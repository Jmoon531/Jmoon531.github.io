### InnoDB行格式？

InnoDB有4种行格式：COMPACT、REDUNDANT、DYNAMIC、COMPRESSED。

#### COMPACT行格式？

COMPACT行格式包含两部分：记录的额外信息 和 记录的真实数据。

记录的额外信息又包括三个部分：

- 变长字段长度列表：所有变长字段的真实数据占用的字节数都逆序存放在变长字段长度列表。如果表的所有列都不是变长的数据类型或者所有列的值都是NULL的话，就不需要变长字段长度列表。

- NULL值列表：允许为NULL的列会逆序将是否为NULL的信息存放在NULL列表中（二进制位为1表示不为NULL，反之则为NULL）。如果表中没有允许为NULL的列，则NULL值列表不存在。

- 记录头信息：记录头信息由固定的5个字节组成。主要包括：

  - deleted_flag：记录是否删除。

  - min_rec_flag：每层非叶子节点中最小的目录项会添加该标记。

  - n_owned：由每组最大的记录会将该组的记录数记录在n_owned中，其余记录的n_owned为0。

  - heap_no：记录在页面中相对位置，即第几个记录。

  - record_type：0表示普通记录、1表示非叶子节点目录项记录、2表示Infimum记录，3表示Supremum记录。

  - next_record：下一个记录的位置。

记录的真实数据除了自己定义的列外，InnoDB还会为每个记录默认添加几个隐藏列：

- row_id：主键id，如果用户没有指定主键，那么会默认添加一个名为row_id的隐藏列作为主键。
- trx_id：事务id，修改该记录的事务id。
- roll_pointer：回滚指针，指向undo日志形成版本链，作用是事务回滚和MVCC。

#### DYNAMIC行格式？

InnoDB默认的行格式是DYNAMIC，它与COMPACT行格式唯一的不同是在处理溢出列时采取的方案。对于COMPACT行格式，只会在真实数据处存储该列前768个字节，然后紧接着用20个字节存储指向溢出页的地址和剩余字节数。对于DYNAMIC行格式，只会在记录的真实数据处用20个字节存储执行溢出页的地址和字节数据。

#### 隐藏主键row_id的生成策略？

MySQL在内存中维护一个全局变量，所有没有设置主键的表都用这个变量分配主键，每次分配后会增加1，每当变量值达到256的倍数时，就会把值刷到系统表空间7号页面的Max Row ID属性中，这个属性占8个字节，系统下次重启时会把Max Row ID加载到内存并加上256作为下一次分配的起始值，这样可以避免每次自增时都要写磁盘，并保证不重复分配。

### InnoDB页结构？

页的大小为16KB，InnoDB以页作为磁盘和内存之间交互的基本单位。InnoDB为了不同的目的设计了很多种不同类型的页，比如：存储表空间头部信息的页、存储段信息的页、存储区信息的页、存储系统信息的页、存储事务信息的页、undo日志页、溢出页、索引页。用来存放数据的页就是索引页，不过溢出页是一种单独类型的页。

一个页由7个部分组成：

- File Header：File Header是各种类型页面的通用组成部分，占用固定的38个字节，主要包含页号、上一个页的页号、下一个页的页号、页的类型、页所属表空间ID、校验和。
- Page Header：存储索引页的一些信息，比如页面的记录数、Free Space的地址偏移、删除记录组成的单项链表的头节点的偏移等。如果是根节点页面还会存叶子节点段和非叶子节点段的INODE Entry位置。
- Infimum + Supremum：两个虚拟记录，Infimum比其他所有记录都小，Supremum比其他所有记录都大。
- User Records：用户记录存储的地方。
- Free Space：页面尚未使用的地方。
- Page Directory：页目录，槽（slot）存放的地方，每个槽占2个字节。
- File Trailler：File Trailler也是各种类型页面的通用组成部分，存储页面的校验和。

#### 记录在页面中是如何存储的？

所有记录都通过行格式中的next_record串成一个链表，Infimum是这个链表的第一个节点，Supremum是这个链表的尾节点。next_record指向的位置是 记录的额外信息 和 记录的真实数据 中间的位置，这也就解释了记录的额外信息为什么要逆序存放。

链表上的记录会被划分成几个组，每个组最后一条记录（也就是组内最大的那条记录）的头信息中的n_owned属性会存储该组有多少条记录，这条记录的页内偏移地址会被存储在Page Directory（页目录）处，每个地址称为槽（slot），这么做的目的是为了使用二分法实现快速查找。需要注意的是，Infimum记录只能单独作为一组，所以Infimum记录的n_owned的值为1，Supremum记录所在的分组的记录在1~8条之间，其余分组的记录数在4~8条之间，当一个分组有8条记录之后再插入一条，会将该分组中的记录拆分成两个组，一个组是4条记录，另一个组则是5条记录。

如果某个记录被删除了，则会从这个链表中剔除，然后所有被删除的记录通过next_record串成一个垃圾链表，后面可以重用这部分空间。

### InnoDB如何管理存储空间？表空间？

InnoDB通过表空间管理存储空间。

#### 表空间

InnoDB有很多不同类型的表空间：系统表空间、独立表空间、通用表空间、undo表空间、临时表空间。注意，MyISAM没有表空间的概念。

一个MySQL服务器只有一个系统表空间，对应是数据目录下的ibdata1文件。每张表都会在数据库对应的目录下创建一个以.idb结尾的文件，这个文件就是独立表空间，表的数据会存储在这个独立表空间中，当然也可以修改配置将表的数据存到系统表空间中。

表空间的结构：表空间 -> 组 -> 区 -> 页。一个页16KB，连续64个页是一个区，即区的大小是1M，每256个区被划分为一个组。InnoDB不用页而是用区作为分配单位的原因是尽可能连续分配空间以减少随机IO。

段：段是空间管理的逻辑单位，一个索引分成两个段：一个叶子节点段 和 一个非叶子节点段。引入段的目的是将 叶子节点 和 非叶子节点 空间的分配和回收分开。一般来说InnoDB每次会将一个完整的区分配给段，但是表很小会填不满一个区，那就会造成存储空间的浪费，所以InnoDB引入了碎片区，碎片区直属于表空间，不属于任何一个段，因此在为一个段分配空间的时候首先会从碎片区中分配单个页面，当分配完32个页面之后再分配完整的区。每个段对应一个INODE Entry，作用是管理段中区的分配和回收，INODE Entry放在表空间的第三个页面中。INODE Entry的位置又如何找到？每个索引的根节点页面中保存着叶子节点段和非叶子节点段的INODE Entry位置。

XDES Entry：每一个区都对应一个XDES Entry（extent descriptor），大小40个字节，作用是管理区中的页面的分配和回收，每个组的所有区的XDES Entry都放在该组的第一个页面中。每个XDES Entry有4中状态：FREE（空闲的区）、FREE_FRAG（有剩余空闲页面的碎片区）、FULL_FRAG（没有剩余空间的碎片区）、FSEG（附属于某个段的区）。前三种直属于表空间的区会分别串成三个链表，链表的基节点放在第一个组的第一个区的第一个页面中（也就是表空间的第一个页面）。第四种状态的区隶属于某一个段，在段中同样又可以将区分为三种类型并串成三个链表：FREE链表（页面全部空闲的区）、NOT_FULL链表（页面部分空闲的区）、FULL链表（没有空闲页面的区），这三个链表的基节点放在INODE Entry中。

总结：段是空间管理的逻辑单位，页是空间管理的最小物理单位，类似于操作系统的内存管理，区是为了连续分配而引入的空间管理的物理单位。

#### 系统表空间的结构？数据字典？

系统表空间比独立表空间多用了5个页面来描述系统属性，其中最重要的是第7个页面中记录的数据字典信息。系统的元数据信息包括9张系统表，其中有4张表尤其重要，只要把这4张表的根节点找到，那么所有的表（包括用户建的表）的根节点就都可以找到了，这4张表的根节点所在的页号就保存在系统表空间的第7页面中。

#### 如何分配和回收碎片区中的页面？

找到表空间第一个页面中的三个XDES Entry链表的头节点后就可以很简单地完成分配和回收碎片区的页面。

#### 如何分配和回收段中的页面？

首先在系统表空间的第7个页面中找到4张系统表的根节点，然后从4张系统表中找到段所在索引的根节点，再接着从根节点页面Page Header中找到叶子节点段和非叶子节点段的Segment Header（INODE Entry地址），然后找到INODE Entry，从而就拿到了三个XDES Entry链表的基节点，之后就可以很简单地完成段中页面的分配和回收。

### 索引

#### InnoDB的索引方案？聚簇索引？

聚簇索引是一个B+树，叶子节点存储的是完整用户记录，非叶子节点存储的是主键+页号组成的目录项记录。页内的记录按照主键的大小顺序排成一个单向链表，同一层级之间的页面按照主键大小排成一个双向链表。InnoDB存储引擎会自动创建聚簇索引，聚簇索引就是数据的存储方式。

#### 二级索引？联合索引？

使用非主键列建立的B+树索引称为二级索引，该列也称为查找键，它的非叶子节点上存的是查找键+主键+页号，叶子节点不像聚簇索引那样存的是完整的用户记录，而是查找键+主键，所以在二级索引上最后查到是主键，不是完整的记录，如果要拿到完整的记录，就还需要**回表**，也就是根据主键在聚簇索引上再查一遍。

联合索引就是同时以多个列建立的二级索引。

一个B+树索引的根节点自创建之日起就不会再移动，也就是页号不会再改变，这个页号会被存储在那4张系统表中，也就是存在数据字典中。

#### MyISAM的索引方案？

MyISAM使用的是B 树，索引和数据分开存储。数据按插入顺序单独存储在一个文件中，并不会按照主键排序。为主键创建的索引单独放在索引文件中，索引节点不是完整的用户记录，而是主键+地址。所以MyISAM没有聚簇索引和二级索引之分，也没有回表一说，或者说MyISAM的索引都是二级索引，利用索引的查询都得回一次表。

#### 索引的代价？

- 空间：额外占用存储空间。
- 时间：增删改需要维护索引；查询时成本分析耗时增加。

#### 索引的作用？

索引可用于查找，注意事项：

- 索引失效：

  - 使用联合索引时不满足最左前缀原则，即跳过了前面的索引列。

  - 在索引列上做计算、使用函数 或 类型转换（手动或自动的）。

  - 模糊查询like以%开始。
  - 回表代价太大导致不走索引，典型场景就是索引列重复值过多。

- 索引列的重复值不宜过多，因为这会导致回表代价大，可能最终执行计划不会用到这个索引，但是却要一直维护这个索引。
- 索引列占的存储空间越小越好，因为数据类型越小，索引占的存储空间也就越小，磁盘IO的性能损耗也就越小。这个建议对于主键尤为重要，因为其他二级索引的每条记录也会存储主键。
- 为列前缀建索引：如果字符串很长可以只把字符串前几个字符放到索引中。
- 不要建冗余和重复的索引。比如给联合索引的第一个列又建立了一个索引，这个索引就是重复索引。

索引用于排序，注意事项：

- 使用联合索引时，order by后面列的顺序必须和联合索引列的顺序一致，可以只使用索引列中左边连续的列进行排序，中间不能断，除非前面的列已经在where条件中限制死了，比如`select * from single_table where key_part1='a' and key_part2='b' order by key_part3`或者`select * from single_table where key_part1='a' and key_part2='b' order by key_part3, id`。
- ASC、DESC不能混用，要么都按ASC排序，要么都按DESC排序。
- 排序的多个列都分别建了索引，那么这些索引均不能用于排序。
- 搜索条件的索引列和排序列不同时无法使用索引列排序。比如`select * from single_table where key1='a' order by key2`。
- 排序列不能做计算或者使用函数。

索引用于分组，注意事项：

- 和使用索引排序一样。

#### 回表的代价？

二级索引中的主键id不是连续的，回表会造成大量的随机IO，所以需要回表的记录数越多，使用二级索引查询的性能也就越低，于是可能使用全表扫描的代价更低。MySQL可以设置先查二级索引，等获取到全部主键id之后再排序，最后再回表（MRR，Disk-Sweep Multi-Range Read，多范围读取）。如果没有开启这项功能，那么查询过程是，每次查二级索引获取主键id后都会回表。

#### 覆盖索引？

select、where、order by、group by 后面的列都在某一个索引中，这种查询方式就是覆盖索引。不一定非得从根节点搜索的查询才叫覆盖索引，直接从叶子节点进行全表扫描的查询也可以叫覆盖索引。覆盖索引可以完全避免回表，所以如无必要，不要`select *`。

#### 索引下推？

扫描二级索引时，如果有其它没有对形成该扫描区间起到作用但是也可以在该索引上提前判断的条件，开启索引下推后存储引擎会提前判断，避免回表后在server层判断，达到减少回表次数的目的，这就是索引下推。索引下推这个特性只适合需要回表的二级索引，不适合聚簇索引和覆盖索引。索引下推只适用于SELECT，不适用UPDATE和DELETE中的WHERE条件。比如：

- `select * from s1 where key_part1 = 'a' and key_part3 = 'a';` ，二级索引中的联合索引，where条件没有连续使用索引列，从中间断了，如果开启了索引下推，那么存储引擎会立刻判断二级索引是否满足后面过滤条件，然后再执行回表，如果没有开启索引下推，那么会先执行回表，获取到完整记录后再在server层判断是否满足后面的过滤条件。`select key_part2 from s1 where key_part1 = 'a' and key_part3 = 'a';`这种属于覆盖索引，`key_part3 = 'a'`这个条件会在server层判断，不会用到索引下推。这是因为从二级索引回表到聚簇索引的整个过程都是在存储引擎层，回表后把完整的记录返回给server层判断剩下的条件，而索引下推只发生在存储引擎层的回表过程，所以覆盖索引不会用到索引下推。
- `select * from s1 where key1 > 'z' and key1 like '%b';`，这种情况也可应用索引下推。

EXPLAIN的Extra列有`Using index condition`表示使用到了索引下推，但是其他使用二级索引需要回表的常规情况（除了使用唯一二级索引进行等值比较）也会使用`Using index condition`的提示，所以MySQL将除了使用唯一二级索引进行等值比较的情况外，其他使用二级索引后需要回表的情况都叫索引下推，设置开启索引下推只是在回表时额外增加上面的逻辑而已。

#### 自适应哈希索引？

InnoDB 存储引擎会在后台默默地监控对索引的查询操作，当发现某些索引的等值查询足够频繁时，InnoDB 会自动在内存中基于这些索引构建哈希索引。哈希索引对于等值查询具有非常高的效率，因为它通过哈希函数直接定位到数据所在的位置，而不像 B - Tree 索引那样需要从根节点开始层层遍历。这种哈希索引是自适应的，如果查询模式发生变化，例如某个原本频繁使用等值查询的索引现在很少被这样访问，或者数据分布发生了较大的变化，InnoDB 会自动调整哈希索引，可能会减少甚至删除那些不再有用的哈希索引部分。自适应哈希索引存储在Buffer Pool中。

### 查询优化

#### 单表访问方法？

单表访问方法：

- const：通过主键或者唯一二级索引与常数的等值比较来定位一条记录的查询，联合索引需要每个索引列都进行等值比较。定位唯一二级索引中值为NULL的记录的这种查询不是const，因为唯一二级索引并不限制NULL值的数量，可能会查出很多记录。
- ref：通过普通二级索引与常数的等值比较的查询，不需要联合索引每个索引列都进行等值比较。这种查询搜索到第一条记录后还需要沿着单向链表继续向后搜索。所以查询唯一二级索引中值为NULL的记录的访问方法是ref。
- ref_or_null：二级索引与常数进行等值比较的同时还查询二级索引中值为NULL的记录。
- range：使用索引查询时，对应的扫描区间为若干个单点扫描区间或者范围扫描区间的访问方法称为range。前面三种访问方法都是等值比较，一个等值比较是一个单点扫描区间，range需要多个单点扫描区间，或者范围查询，但是范围是(-无穷大，+无穷大)的查询不是range。
- index：全表扫描二级索引不需要回表的访问方法称为index。注意，这里在二级索引上的扫描范围是(-无穷大，+无穷大)。
- index_merge：使用Intersection、Union、Sort-Union这3种索引合并方式执行的查询。
- all：全表扫描聚簇索引的访问方法称为all。

#### 索引合并？

- Intersection索引合并：and条件使用到的所有索引都是二级索引，并且从每个索引中获取到的二级索引记录都是按照主键值排序的，这个时候会对多个索引查询结果取交集。
- Union索引合并：or条件使用到的所有索引都是二级索引，并且从每个索引中获取到的二级索引记录都是按照主键值排序的，这个时候才会对多个索引查询结果取并集。
- Sort-Union索引合并：or条件使用到的所有索引都是二级索引，但是获取到的二级索引记录的主键值并不都是排好序的，那么会将从各个索引中扫描到的主键值排序，然后再使用Union索引合并。

只有Sort-Union索引合并，没有Sort-Intersection索引合并（MariaDB有Sort-Intersection索引合并）的可能原因是：如果不使用Union索引合并，只使用其中一个索引，那么最终还是需要全表扫描判断第二个条件是否成立，相当于两个索引都没有用到，所以引入Sort-Union索引合并是很有用的。但是对于Intersection索引合并而言，如果只使用一个索引，另一个判断条件可以再回表之后进行，其成本可能比使用两个索引的Sort-Intersection索引合并更低。

#### 多表连接的方法？

- 嵌套循环连接（Index Nested-Loop Join，NLJ）：每次查询驱动表得到一条记录之后就根据这条记录立刻查询被驱动表的方式称为嵌套循环连接。
  - 驱动表只访问一次，被驱动表访问的次数取决于对驱动表单表查询后的结果集中有多少条记录。
  - 嵌套循环连接会使用索引加快驱动表和被驱动表的查询速度，如果没有索引，那么直接选择基于块的嵌套循环连接方法。
  - 通过主键或者唯一二级索引对单表进行常数等值查询的访问方法称为const，而在连接查询中对被驱动表通过主键或者唯一二级索引进行等值查询的访问方法称为eq_ref。
  - 每次查询驱动表得到一条记录后就立刻访问被驱动表可能会造成大量的随机IO，和回表的代价一样，所以这里也可以使用MRR进行优化，将记录按关联键排序之后再一次性访问被驱动表，这种在连接时使用MRR的方法称为Batched Key Acess(BKA)算法。
- 基于块的嵌套循环连接（Block Nested-Loop Join，BNL）：在执行连接查询前申请一块固定大小的内存（Join Buffer，连接缓冲区），先把若干条驱动表结果集中的记录装在这个Join Buffer中，然后扫描被驱动表，每一条被驱动表的记录一次性地与Join Buffer中的多条驱动表记录进行匹配。
  - 最好是驱动表能一次全部装进Join Buffer中，也就是小表驱动大表。
- Hash JOIN：Hash Join将驱动表和被驱动表按关联键hash之后再做连接，只适用于等值连接，Hash Join最大的优势应该是可以并行Join。
  - Hash join使用的也是Join Buffer，最好是驱动表能一次全部装进内存，如果内存放不下会溢写到磁盘文件。
  - MySQL 8.0.18 引入了Hash Join，对于MySQL来说，Hash join属于在BNL（基于块的嵌套循环连接）算法的一部分，开启BNL（`block_nested_loop=on`），MySQL会尽可能先使用Hash join，如果用不了Hash join再使用传统的BNL。

> 参考：[MySQL中的Join 的算法（NLJ、BNL、BKA）-阿里云开发者社区](https://developer.aliyun.com/article/1307704)
>
> [技术分享 | Hash join in MySQL 8 - 个人文章 - SegmentFault 思否](https://segmentfault.com/a/1190000021054708#item-1-2)
>
> [MySQL :: MySQL 8.4 Reference Manual :: 10.2.1.4 Hash Join Optimization](https://dev.mysql.com/doc/refman/8.4/en/hash-joins.html)

#### 查询优化？

查询优化有两种手段：

- 基于成本进行优化：从多个执行计划中找出成本最低的方案。
- 基于规则进行优化：依据一些规则对查询进行重写。

#### 查询成本？

MySQL执行成本由两方面组成：

- IO成本：InnoDB无法准确预测页面是否已经加载到内存，所以默认读取一个页面的成本是1.0。
- CPU成本：检测一条记录是否符合搜索条件的成本默认是0.2，不管是否存在where条件都一样。

#### 单表查询成本？

找出成本最低的执行计划的一般步骤如下：

1. 计算全表扫描的代价。
2. 计算使用不同索引执行查询的代价。
3. 找出成本最低的方案。

全表扫描的代价是下面两部分之和，其中表中的记录数n_rows可以直接从InnoDB收集的统计数据中获取，是一个估计值：

- IO成本 = 聚簇索引占用的页面数 * 1.0
- CPU成本 = 记录数 * 0.2

使用二级索引的代价等于以下4部分之和：

- 二级索引的IO成本：查询优化器简单粗暴地认为读取二级索引的一个扫描区间的IO成本等于读取一个页面的IO成本。
- 二级索引的CPU成本：计算某个扫描区间到底包含多少条记录的过程是，首先找到扫描区间最左边和最右边的记录，然后沿着叶子节点页面的双向链表向右最多读取10页面，直至遇到最右记录所在的页面，如果提前遇到就提前结束，如果读完10个页面还遇不到也不再读了，最后将页面Page Header中记录该页面的记录数的数值加起来，如果最右记录在10个页面之内，那么记录数之和就是需要回表的记录数，如果最右记录在10个页面之外，那就将记录数据之和除以10得到平均每个页面的记录数，之后乘以最左记录和最右记录之间相隔的页面数，即可预估需要回表的记录数。二级索引的CPU成本就等于需要回表的记录数 * 0.2。
  - 如何得到最左记录和最右记录之间相隔多少个页面？可以分别从最左记录和最右记录向上递归直至找到同一个非叶子节点，就可以根据非叶子节点中的目录项计算得到他俩之间相差的页面数。
- 回表的IO成本：查询优化器简单粗暴的认为每次回表都相当于访问了一个页面，所以回表的IO成本等于回表的记录数 * 1.0。
- 回表的CPU成本：等于回表的记录数 * 0.2。

#### 多表连接的成本？  

两表连接查询总成本  = 单次访问驱动表的成本 + 驱动表扇出值 * 单次访问被驱动表的成本。（这个成本计算方式很粗略的，对于基于索引的嵌套循环连接大致适用，但是对于基于块的嵌套循环连接和哈希连接就不太适用）

扇出（fanout）是指驱动表中满足where条件的记录数，真实的扇出值只能在真正执行完驱动表查询之后才能得到，所以计算连接成本时扇出值只能靠猜测（通过一些启发式规则进行猜测），这个猜测过程称为条件过滤（Condition Filtering）。

对于外连接来说，驱动表是固定的，不能轻易互换（某些特殊情况下外连接可以优化为内连接），但是对于内连接来说，驱动表和被驱动表可以互换，所以需要选取不同的表作为驱动表来计算内连接的成本，最后选择成本最低的方案。

多表内连接有n的阶乘种连接顺序，MySQL会通过一些方法提前优化掉某些连接顺序，压根就会计算这些连接顺序的成本，从而减少成本分析的成本。

#### InnoDB统计数据是如何收集的？

InnoDB会把统计数据保存到两个表中，InnoDB可以设置定期自动更新统计数据，也可以手动调用ANALYZE TABLE命令更新统计信息，还可以手动update这两张表：

- innodb_table_stats：存放关于表的统计数据，每一条记录对应着一个表的统计数据。
- innodb_index_stats：存放关于索引的统计数据，每一条记录对应着一个索引的一个统计项的统计数据。

innodb_table_stats中有三个比较重要的列：

- n_rows：表中的记录数。
- clustered_index_size：表的聚簇索引占用的页面数量。
- sum_od_other_index_sizes：表中其他索引占用的页面数量。

n_rows的值是这样估计的：按照一定的算法从聚簇索引中选取几个叶子节点页面，统计每个页面中包含的记录数量，然后计算平均每个页面包含的记录数，之后再乘以全部叶子节点数据，得到的结果就是n_rows值。n_rows值的精度取决于统计时采样的页面数量，这个采样页面数量可以配置，默认值是20。

clustered_index_size和sum_od_other_index_sizes可以通过统计各个索引对应的叶子节点段和非叶子节点段分别占用的页面数量之后加和得到。段对应的INODE Entry结构可以在根节点页面Page Header中找到，根节点页面的页号可以在数据字典中找到，数据字典的根节点页面可以在系统表空间的第7个页面中找到。

#### 基于规则优化？

基于规则的优化有：条件化简、外连接消除、子查询优化。

#### 条件化简？

- 移除不必要的括号
- 常量传递：a=5 and b>a 可以化简成 a=5 and b>5，如果是or，则不可以常量传递。 
- 移除没用的条件：优化器会移除恒为TRUE或FALSE的条件表达式。
- 表达式计算：如果表达式中只包含常量，它的值会被先计算出来。
- 常量表检测：针对表中只有一条记录或者一条记录都没有，又或者使用主键、唯一二级索引进行等值匹配的情况，因为这两种查询花费的时间很少，可以忽略，所以把通过这两种方式查询的表称为常量表，查询优化器会先执行对常量表的查询，然后把查询中涉及到该表的条件都换成常数，最后再分析其余表的查询成本。

#### 外连接消除？

在外连接后的where条件中限定被驱动表的列不为NULL，或者隐含地包含这个意思，那么外连接就和内连接没有区别了，将外连接转换成内连接的好处是优化器可以评估不用连接顺序的成本，从中选出成本最低的连接顺序来执行查询。把在外连接中指定where包含被驱动表列不为NULL的条件成为空值拒绝（reject-NULL）。

#### 子查询优化？

子查询最常出现在：select子句中、from子句中、where子句中。

子查询按返回的结果集可分为：

- 标量子查询：只返回一个单一值的子查询。
- 行子查询：返回一条记录的子查询。
- 列子查询：返回一列数据的子查询，这一列包含多条记录。
- 表子查询：返回多条记录的子查询。

子查询那与外层查询的关系可分为：

- 不相关子查询：子查询不依赖外层查询的值，可以单独执行。
- 相关子查询：子查询的执行需要依赖外层查询的值。

不相关的标量子查询和行子查询可直接先执行，不需要优化，子查询优化的重点是：出现在from子句中子查询、where中IN子查询、where中EXISTS子查询。

#### FROM子查询优化？

FROM后面的子查询称为派生表。FROM子查询优化有两种：

- 把派生表物化，这里的物化是指把子查询的结果保存到临时表中，没有去重。
- 把派生表和外层查询合并，直接提出来，免去创建和访问临时表的成本。

MySQL在执行带有派生表的查询时会优先尝试把派生表和外层查询进行合并，如果不行再把派生表物化，然后执行查询。

#### IN子查询优化？

IN子查询不管是否是相关子查询，都是两张表的之间的查询，既然是两张表之间的关系，那就可以转换成连接查询，从而使用连接查询的查询优化方法，但是IN子句有去重的效果，但连接查询不行，连接查询关联键重复会导致数据翻倍，所以MySQL针对IN子查询提出了半连接（semi-join），即a表与b表进行半连接只关心b表中是否存在与之匹配的记录，而不关心具体有多少记录能匹配，最终的结果只保留a表中的记录。半连接的实现方式有如下几种，基本上都是在连接查询的基础上的改造：

- Semi-join Materialization（半连接物化）：先把IN中的不相关子查询物化（这里的物化是指把子查询的结果存到具有唯一性的临时表中），然后再将外层查询的表与物化表进行连接。对于相关子查询不能使用这种半连接实现方式。
- Table pullout（子查询表上拉）：当子查询的查询列表（select后面的列）只有主键或者唯一索引列时，子查询的结果本来就具有唯一性，所以可以直接将子查询中的表上拉与外层查询中的表join。
- Duplicate Weedout（重复值消除）：直接将子查询中的表上拉与外层查询中的表join，之后为了消除重复，将结果集按主键id去重，去重的方法是按主键id建一张临时表。
- LooseScan（松散扫描）：子查询的查询列表是某个索引的键值，那么可以将子查询中的表作为驱动表，每次扫描到值相同的键值时只取第一个，然后根据键值查询外层的被驱动表。
- FirstMatch（首次匹配）：首次匹配是一种最原始的半连接执行方式，即先取一条外层查询的记录，然后执行子查询判断是否满足条件，如此循环，和嵌套循环连接一样。

并非所有IN子查询都可以使用半连接，半连接的适用条件如下：

- 使用的是IN，而不是NOT IN。NOT IN使用反连接（antijoin）。
- IN必须在外层查询中的WHERE或者ON子句中，不能位于SELECT中。
- 外层查询可以有其他条件，但是必须使用and连接，不能使用or操作符。
- 子查询必须是一个单一的查询，不能是由UNION连接起来的若干个查询。
- 子查询中不能包含GOUP BY、HAVING或者聚集函数。

~~对于不能转换成半连接的子查询，MySQL还有两个优化方法，最后会从中找出成本更低的方法执行：~~

- ~~对于不相关子查询，可以把它物化后再参与外层查询，比如NOT IN子查询。~~
- ~~无论是相关还是不相关子查询，都可以把IN子查询转换成EXISTS子查询。~~

应该所有IN子查询都可以使用半连接吧，因为FirstMatch（首次匹配）就是兜底使用的方法，任何IN子查询都能用。

#### EXISTS子查询优化？

- ~~如果子查询是不相关的，可以直接先执行子查询。~~
- ~~如果子查询是相关的，那么就是使用嵌套循环连接的方式执行，其中外层查询的表是驱动表，EXISTS子查询的表是被驱动表。同样如果有索引可以加快查询的速度。NOT EXISTS使用反连接（antijoin）。~~

EXISTS子查询和IN子查询一样，可以优化成半连接。NOT EXISTS优化成反连接。

### EXPLAIN

explain可以查看查询语句的查询计划，输出的各个列的作用如下：

```
mysql> explain select * from user;
+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+
| id | select_type | table | partitions | type | possible_keys | key  | key_len | ref  | rows | filtered | Extra |
+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+
|  1 | SIMPLE      | user  | NULL       | ALL  | NULL          | NULL | NULL    | NULL |    5 |   100.00 | NULL  |
+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+
1 row in set, 1 warning (0.02 sec)
```

- table：explain输出的每一行都对应着一张单表的访问方法，table列是该表的表名。
- id：每个select关键字都对应一个唯一的id。
  - 对于连接查询，每个表都会输出一行，但是这些行的id是相同的，出现在前面的表是驱动表，出现在后面的表是被驱动表。
  - 查询优化器可能对子查询进行重写，转成半连接查询，这个时候外层select和子查询的select的id值是相同的。
  - UNION子句会建内部临时表去重，此时这个临时表对应行的id为NULL。

- select_type：每张单表查询在整个查询语句中扮演的角色。select_type取值如下：
  - SIMPLE：查询语句中不包含UNION或者子查询的查询都是SIMPLE。
  - PRIMARY：对于包含UNION、UNION ALL或者子查询的大查询，最左边那个查询的select_type就是PRIMARY。
  - UNION：UNION子查询中除最左边的子查询外，其余子查询的select_type就是UNION。
  - UNION RESULT：使用临时表完成UNION查询的去重工作，临时表的select_type就是UNION RESULT。
  - SUBQUERY：IN子查询不能转换成半连接，但是是不相关子查询，并且查询优化器决定物化该子查询，那么该子查询的select_type就是SUBQUERY。
  - DEPENDENT SUBQUERY：IN子查询不能转换成半连接，最后查询优化器决定转换成EXISTS子查询，那么该子查询的select_type就是DEPENDENT SUBQUERY。
  - DEPENDENT UNION：子查询里面包含UNION会出现这个取值。
  - DERIVED：以派生表物化的方式执行子查询，派生表对应的子查询的select_type就是DERIVED。
  - MATERIALIZED：选择将子查询物化后与外层查询进行连接的子查询的select_type就是MATERIALIZED，应该是Semi-join Materialization（半连接物化）的专属。
- type：单表的访问方法。常用取值就是前面介绍的单表访问方法。
- possible_keys和key：possible_keys表示单表查询可能用到的索引；key表示实际用到的索引。possible_keys并不是越多越好，可以使用的索引越多，计算查询成本的时间就越长。
- key_len：使用的索引的列的长度。如果只是单个列的索引，那么就是这个列的长度，如果是联合索引，实际使用了联合索引中多少个列，那key_len就是这些列的长度之和。key_len主要作用就是为了让我们知道在使用联合索引时具体使用到了多少个列。
- ref：当单表访问方法是const、ref、ref_or_null、eq_ref等时，ref列展示了与索引列进行等值比较的东西。
- rows：扫描区间预计要扫描的记录数。
- filtered：扫描区间中满足其余搜索条件的记录数的百分比。对于连接查询的驱动表，filtered * rows就是驱动表的扇出数。
- Extra：查询的额外信息。常见的比较重要的有：
  - No tables used：没有FROM子句时是这个提示。
  - Zero limit：limit子句的参数为0时是这个提示。
  - Impossible WHERE：where条件永远为FALSE。
  - Using Index：使用覆盖索引执行查询。
  - Using index condition：使用到了索引下推。
  - Using where：搜索条件需要在server层判断。
  - Using join buffer(Block Nested Loop)：使用基于块的嵌套循环连接算法执行连接查询。
  - Using join buffer (hash join)：使用Hash join。
  - Using intersect(...)、Using union(...)、Using sort_union(...)：使用索引合并的方式执行查询，括号中表示要进行合并的索引。
  - Using filesort：要对结果集中的记录排序。无论是在内存中还是要借助磁盘统称为filesort。
  - Using temporary：使用临时表完成去重、排序等功能。
  - Start temporary，End temporary：当IN子查询的半连接策略是Duplicate Weedout时，会建临时表给结果去重，此时驱动表的Extra显示Start temporary，被驱动表的Extra显示End temporary。
  - LooseScan：IN子查询的半连接策略是LooseScan，驱动表的Extra显示LooseScan。
  - FirstMatch(tbl_name)：IN子查询的半连接策略是FirstMatch，被驱动表的Extra显示FirstMatch(tbl_name)。

EXPLAIN还支持其他格式的执行计划，使用FORMAT限制，比如使用JSON格式：`EXPLAIN format=json`，使用tree格式：`EXPLAIN format=tree`。使用JSON格式的执行计划可以直观地看到每一步的查询成本。

EXPLAIN ANALYZE 会实际执行查询语句，提供实际执行查询所花费的时间。

### Buffer Pool

Buffer Pool被划分成若干页面的一片内存，Buffer Pool中的页面称为缓存页，和磁盘上表空间使用的页面大小一样，都是16KB，每个缓冲页对应一个控制块，用于管理缓冲页，控制块包含缓冲页对应的表空间编号、页号、缓冲页在Buffer Pool中的地址、各种链表节点等，所以BUffer Pool主要包含两部分：缓冲页 和 控制块。

InnoDB可以配置多个BUffer Pool实例，目的是提高并发处理能力，因为修改链表需要加锁。

InnoDB以chunk为单位向操作系统申请空间，支持在运行过程中调整BUffer Pool的大小。chunk是指MySQL的配置项innodb_buffer_pool_chunk_size。

InnoDB使用各种链表来管理Buffer Pool的缓冲页，这些链表节点都在控制块中，主要是这三个链表：

- free链表：包含所有空闲的缓冲页。
- flush链表：被修改过但还没有刷盘的缓冲页称为脏页，所有脏页都被放在flush链表中。
- LRU链表：记录了所有正在使用的缓冲页，其中最近最久未使用的缓冲页会被放在LRU链表的尾部，当缓冲页不足时，可以直接淘汰尾部缓冲页。

如何快速定位一个页是否在缓冲区中？使用哈希表，表空间+页号作为key，缓冲页控制块的地址作为value。在访问某个页面时，先从哈希表中根据表空间+页号找，如果没有找到就从free链表中取一个空间缓冲页。

free链表 + LRU链表 包含了所有缓冲页，flush链表只是LRU链表的子集，哈希表和LRU链表包含的缓冲页范围是一样的。

#### 如何避免预读和全表扫描对LRU链表的影响？

预读和全表扫描对LRU链表的影响：预读的页面后面不一定会用到，全表扫描的页面之后的使用频率也很低，这些页面加载到Buffer Pool中会把LRU链表中使用频率更高的页面淘汰掉。

如何避免：LRU链表分为两个区域：young区 和 old区，old区默认占37%。初次将磁盘中的页加载到缓冲区时会放到old区的头部，并在对应的控制块上记录访问时间，然后下一次访问时检查时间间隔，如果两次访问时间超过设置的阈值（默认是1s）才会将缓冲页放到young区，当Buffer Pool中没有空闲缓冲页时会优先淘汰old区中的缓冲页。要超过一段时间的原因是，一个缓冲页中有很多条记录，每次访问一条记录都算作是一次访问，对于全表扫描的场景，也就会在短时间内访问一个缓冲页多次，所以不能以次数来判定是否将缓冲页加入到young区，解决的办法只能是用时间间隔来判定。

#### 刷写脏页的时机？

- 后台线程会定时从LRU链表old区或者flush链表尾部刷脏。
- 系统繁忙时，首先看LRU链表尾部有没有可以直接释放的未修改的缓冲页，如果没有，可能会出现用户线程直接从LRU链表或者flush链表尾部刷脏。

### redo日志

redo日志用于实现事务的持久性，是WAL技术的实现 。实现事务持久性选择redo日志，而不是直接刷写脏页的原因：

1. 脏页太大，即使只修改了一处，也需要把整页写到磁盘上，而redo日志占用空间更小。
2. 刷写脏页是随机IO，代价太大，而redo日志落盘是顺序IO。

为了保证修改操作的原子性，必须以组的形式记录redo日志，每一组redo日志会以一条特殊的redo日志结尾，崩溃恢复时必须要能读到这条表示结尾的redo日志，否则整组日志都不会恢复，InnoDB把这种不可分割的一组日志称为mtr(mini-transaction)，于是一个事务由多个语句组成，一个语句由多个mtr组成，一个mtr又由多条redo日志组成。

redo日志并不是直接写磁盘，而是先往内存中的log buffer写，待事务提交时再全部刷写到磁盘。log buffer和磁盘文件都由512字节的redo log block组成，不同事务的mtr可以交叉写，但是一个mtr是不能被分开的。log buffer默认16MB。磁盘上的redo日志不像undo日志，redo日志存储空间并不是在表空间中分配的，而是单独的几个文件，默认是2个，每个文件的大小是48M，这2个文件又称为redo日志文件组，redo日志会以循环的方式往文件组中写，满了之后会覆盖掉前面的，能被覆盖的redo日志其对应的脏页必须已经刷写到磁盘了。

#### 如何保证redo日志循环写入时被覆盖日志的脏页已经刷写到磁盘？

InnoDB引入了checkpoint机制，其中有三个关键的变量，不过最重要是checkpoint_lsn，checkpoint_lsn被持久化在第一个日志文件组的前4个block中：

- lsn(log sequence number)：记录当前总共写入log buffer的日志量。
- flushed_to_disk_lsn：记录已经刷写到磁盘的日志量。
- checkpoint_lsn：记录可以被覆盖的日志量。redo日志恢复的起点也是checkpoint_lsn。

这三个变量只会不断递增，而日志文件组的大小是不变的，不过很容易就可以将lsn的值换算成日志文件中位置。

Buffer Pool中的flush链表记录的是被修改过的脏页，在每个链表节点（控制块）中还记录了两个变量：

- oldest_modification ：第一次修改缓冲页的lsn。
- newest_modification：最近一次修改缓冲页的lsn。

第一次修改页面时会把页面放在flush链表的头部，之后每次修改不会调整位置，只会更新newest_modification，所以flush链表中的脏页是按照页面第一次修改时间排序的，所以flush链表的尾部对应的oldest_modification之前的redo日志可以被覆盖，checkpoint机制就是flush链表尾部的oldest_modification赋值给checkpoint_lsn，所以checkpoint并不是一定要从flush链表尾部开始刷脏。

#### redo日志刷盘时机？

- log buffer空间不足时。log buffer容量已使用50%左右时就需要开始刷盘了。
- 事务提交时。为了保证事务的持久性，必须要事务提交时把redo日志刷盘，刷盘成功后才能返回事务提交成功。
- 刷写脏页时会保证先把其对应的redo日志刷盘。
- 后台线程大约每秒刷一次盘。
- checkpoint时。checkpoint时如果flush链表尾部的oldest_modification前的redo日志还没有刷盘，那么就需要先将redo日志刷盘，然后再做checkpoint。
- 服务器正常关闭时。

redo日志刷写线程、checkpoint线程、刷写脏页的线程，这三个通常是不同的线程，但是当数据库系统修改操作特别频繁时，这三项工作可能会都由用户线程做。

#### 从redo日志中崩溃恢复时，如果遇到脏页已经刷盘的情况如何处理？

页面的File Header有一个记录着最近一次修改页面时对应的lsn，如果脏页已经刷盘，凡是日志对应的lsn小于页面的lsn，都不用管。

### undo日志

undo日志用于事务回滚，保证事务的原子性。InnoDB的行格式会自动添加两个隐藏列，一个是**trx_id**，表示事务id，只在事务对某张表执行增删改操作时，才会为这个事务分配一个事务id，否则不会分配事务id，默认为0；另一个是**roll_pointer**，指向记录所对应的undo日志的指针。undo日志中也有这两列，通过roll_pointer可以为一条记录形成一个版本链，这也是MVCC实现的关键。                                                                                                                                                                                           

#### 增删改操作对应的undo日志？

INSERT操作对应的undo日志类型是TRX_UNDO_UPD_EXIST_REC，这种日志重点是把主键记录下来。插入的新记录的roll_pointer会指向该条undo日志，trx_id修改成插入这条记录的事务id。

DELETE操作会经历两个阶段：

- 阶段1：也称为delete_mark操作。将记录的deleted_flag标志位设置为1，DELETE操作对应的undo日志是TRX_UNDO_DEL_MARK_REC，然后将被删除记录的roll_pointer和trx_id赋值给undo日志，之后将被删除记录的roll_pointer指向该条undo日志，trx_id修改成删除这条记录的事务id。
- 阶段2：删除事务提交后，purge线程会真正把记录从页面的正常记录链表中删除，加入到垃圾链表中的头部。


UPDATE操作对不更新主键和更新主键的处理方式不同：

- 针对不更新主键的情况，UPDATE操作对应的undo日志是TRX_UNDO_UPD_EXIST_REC，将被更新记录的roll_pointer和trx_id赋值给undo日志，之后将被更新记录的roll_pointer指向该条undo日志，trx_id修改成更新这条记录的事务id。

  不更新主键在具体的操作上又分为两种情况：

  - 就地更新：对被更新的每个列，如果更新前后列占用的存储空间不变，则称为就地更新。

  - 先删除旧记录，再插入新记录：如果更新前后列的存储空间变大或者变小了，则会先真正删除原记录（不是delete_mark操作），然后插入新记录。

- 针对更新主键的情况，会先对原记录作DELETE操作（是delete_mark操作），产生的undo日志是TRX_UNDO_DEL_MARK_REC，然后再做INSERT操作，产生的undo日志是TRX_UNDO_UPD_EXIST_REC，也就是说UPDATE更新主键其实是INSERT和DELETE的组合操作。

上述三种undo日志可以被分为两大类：

- TRX_UNDO_INSERT（insert undo日志）：INSERT操作产生的TRX_UNDO_UPD_EXIST_REC日志属于这种类型。这类日志没有trx_id和roll_pointer两列，所以只会作为版本链的尾节点。
- TRX_UNDO_UPDATE（update undo日志）：DELETE和UPDATE不更新主键操作产生TRX_UNDO_DEL_MARK_REC和TRX_UNDO_UPD_EXIST_REC是这种类型。这类日志有trx_id和roll_pointer两列，通过roll_pointer可以为一条记录形成一个版本链。

区分这两种类型undo日志的原因是，这两种日志会分别存放在不同类型的页面中，当事务提交时，insert undo日志可以直接删除掉，而update undo日志不能，update undo日志还需要为MVCC服务。

#### undo日志存储空间是如何分配和回收的？

undo日志存储在表空间中，对应的页面是类型为FIL_PAGE_UNDO_LOG，与存储数据的页面类型不同。undo页面可以通过保存在其中的链接节点串联成一个链表，两种不同类型的日志（insert undo日志和update undo日志）会被存放在不同的链表中，事务提交后insert undo日志链表可以直接删除，但是update undo日志链表不能，因为其还需要为MVCC服务。又根据事务操作的表的类型不同，一个事务的最多可以有4个链表：普通表的insert undo链表、普通表的update undo链表、临时表的insert undo链表、临时表的update undo链表。每一个undo页面链表都对应着一个段，用于分配和回收链表中页面，该段对应的Segment Header存放在undo链表的第一个页面中，通过Segment Header可以找到段对应的INODE Entry。

那如何找到undo链表的第一个页面？undo链表的第一个页面的页号存放在一个叫Rollback Segment Header的页面中，这个页面一共可以存放1024个undo链表第一页页号，每个页号都称为一个undo slot。Rollback Segment Header页面也称为回滚段，这个页面也使用一个段来管理空间，该段对应的Segment Header就存放在这个页面中，一个页面也要使用一个段是因为要统一概念，即段是空间管理的逻辑单位。

那又如何找到Rollback Segment Header页面的页号？在系统表空间的第5号页面存放了128个Rollback Segment Header页面页号。每个页号包括表空间号和页号，所以每个回滚段都可以放在不同的表空间中。InnoDB规定第0号、第33~127号回滚段属于一类，只能存放普通表的undo日志，其中第0号回滚段必须在系统表空间中，第33~127号可以在系统表空间中，也可以在undo表空间中；第1~32号回滚段属于另一类，只能存放临时表的undo日志，必须在临时表空间（对应数据目录的ibtmp1文件）中。

### MVCC

READ COMMITTED 和 REPEATABLE READ 隔离级别的实现方式是MVCC，而SERIALIZABLE隔离级别的实现与MVCC无关。MVCC实现的关键有两点：版本链 和 ReadView（一致性视图）。

InnoDB聚簇索引的行格式有3个隐藏列：row_id、trx_id、roll_pointer，trx_id表示修改该记录的事务id，roll_pointer指向undo日志。delete和update操作生成的日志也有trx_id和roll_pointer两列，生成这两种undo日志时会将记录的trx_id和roll_pointer赋值给undo日志，从而就形成了版本链。insert操作是一条记录的开始，没有再比insert undo日志更早生成的undo日志，也就是说insert undo日志不需要再指向其他undo日志，所以insert undo日志没有trx_id和roll_pointer，它是版本链的尾节点。

ReadView用于判断版本链中哪个版本时当前事务可见的，它是一个结构体，包含4项内容：

- m_ids：生成ReadView时当前系统中活跃的读写事务的事务id列表。
- min_trx_id：生成ReadView时当前系统中活跃读写事务中最小的事务id，也就是m_ids中的最小值。
- max_trx_id：生成ReadView时系统应该分配给下一个事务的事务id。注意max_trx_id并不是m_ids中的最大值，max_trx_id >= m_ids中的最大值。
- creator_trx_id：生成该ReadView的事务id。

然后利用ReadView通过以下规则在版本链中逐个判断某个版本对当前事务是否可见：

1. 如果版本的trx_id与ReadView的creator_trx_id相同，那么表示事务正在访问它自己修改的记录，所以该版本对当前事务可见。
2. 如果版本的trx_id小于ReadView的min_trx_id，那么表示生成该版本的事务已经提交了，所以该版本对当前事务可见。
3. 如果版本的trx_id大于ReadView的max_trx_id，那么表示生成该版本的事务在当前事务生成ReadView后才开启，所以该版本对当前事务不可见。
4. 如果版本的trx_id在ReadView的min_trx_id和max_trx_id之间，则需进一步判断是否在m_ids中，如果在m_ids中，那么表示生成该版本的事务还没提交，所以当前事务不可见该版本，但如果不在m_ids中，那么表示生成该版本的事务已经提交了，所以该版本对当前事务可见。

READ COMMITTED 和 REPEATABLE READ 之间的区别就是ReadView生成的时机不同，READ COMMITTED在每次读取时事务都会生成一个ReadView，而REPEATABLE READ只在事务第一次读取时生成一个ReadView，往后的每次读取都重复利用这个ReadView。

MVCC只用于READ COMMITTED 和 REPEATABLE READ隔离级别下的SELECT，不适用UPDATE和DELETE的where条件。

#### 事务id如何生成？

只有当事务执行增删改操作之后才会分配事务id，如果事务没有执行过增删改操作，那么就不会该事务分配事务id，直接默认的0作为事务id，对前面判断规则没有影响。

事务id的生成策略和隐藏主键row_id是一样的，MySQL在内存中维护一个全局变量，每次分配后会增加1，每当变量值达到256的倍数时，就会把值刷到系统表空间5号页面的Max Trx ID属性中，这个属性占8个字节，系统下次重启时会把Max Trx ID加载到内存并加上256作为下一次分配的起始值，这样可以避免每次自增时都要写磁盘，并保证不重复分配。

#### 二级索引如何MVCC？

二级索引中的记录没有undo日志，也就没有trx_id和roll_pointer，但二级索引页面的Page Header部分有一个PAGE_MAX_TRX_ID属性，PAGE_MAX_TRX_ID记录着修改该二级索引页面的最大事务id。当select语句访问某个二级索引页面时，会比较ReadView的min_trx_id是否大于PAGE_MAX_TRX_ID，如果大于，表示事务已经提交，二级索引页面中所有记录都对该事务可见，否则必须回表后再判断可见性，所以覆盖索引并不一定不用回表。

#### purge?

事务提交之后insert undo日志页面链表可以直接删除，但是update undo日志不能，必须等到系统中没有一个ReadView能访问其中任意一个版本才行，负责清除update undo日志的线程叫purge。purge线程并不是简单删除update undo页面就完事了，update undo页面链表第一个页面中的TRX_UNDO_DEL_MARKS属性标识了本组undo日志中是否有因delete mark操作产生的日志，如果有的话，那么purge线程还需要执行delete操作的第二个阶段，也就是把被删除的记录加入到垃圾链表中，并调整页面中属性的值。

#### 长事务的危害？

- 对于可重复读事务，第一次生成的ReadView要一直保持到事务结束，如果该事务迟迟不结束，那么就会导致其它早已提交的事务的update undo日志的空间一直无法被释放。
- undo slot有限，导致无法创建新的增删改事物，不过undo slot有128*1024=131072个，很难达到这个限制。

### 锁

为了避免脏写，在所有隔离级别下写操作必须加锁，而读操作则不同，在READ UNCOMMITTED、READ COMMITTED、REPEATABLE READ隔离级别下，为了提升并发度（可用性），读操作都不加锁，放弃了部分读写一致性，但是在SERIALIZABLE隔离级别下，读操作会加锁，保证了读写一致性。

锁定读操作比较特别，可以在读数据时显式主动加锁：

```
SELECT ... LOCK IN SHARE MODE; -- 对读取记录加S锁
SELECT ... FOR UPDATE; -- 对读取记录加X锁
```

#### InnoDB存储引擎锁的类型？

锁可以按下面两个维度区分：

- 按锁的粒度区分：表锁、行锁。
- 按读写操作分：S锁（share，共享锁）、X锁（exclusive、排它锁）。

表锁：

- S锁和X锁：直接锁整张表的情况基本遇不到，除非显式用MySQL提供的锁表语句加锁。
- 意向锁（Intention锁，IS锁和IX锁）：对表中某些记录加S锁式之前需要先在表级别加IS锁，对表中某些记录加X锁式之前需要先在表级别加IX锁。IS锁和IX锁是为了后续加表锁时，避免使用遍历的方式查看表中有没有上锁的记录。IS锁和IX锁是相容的，IX锁和IX锁也是相容的。
- AUTO-INC锁：如果表中某列添加了AUTO_INCREMENT属性，执行insert语句就会添加表级别AUTO_INC锁。

行锁：

- Record Lock：记录锁。仅给一条记录加锁。
- Gap Lock：间隙锁。给记录前面的间隙加锁，防止插入幻影记录。给Supremum记录加gap锁可以防止其他事务向正无穷大的区间中插入记录。
- Next-Key Lock：临键锁。记录锁和间隙锁的合体，既可以锁住记录，又可以锁住记录前面的间隙。
- Insert Intention Lock：插入意向锁。事务插入记录遇到Gap Lock时会生成一个插入意向锁，表明自己正在等待插入，等待被持有Gap Lock锁的事务唤醒。插入意向锁不会阻塞其他任何锁。
- 隐式锁：insert操作如果没有碰到Gap锁就不会在新插入的记录上生成任何锁结构，而是利用隐式锁来避免并发问题。隐式锁主要是针对insert操作（UPDATE和DELETE更新二级索引时加的也是隐式锁），可以减少生成和维护锁结构的开销。

Record Lock、Gap Lock、Next-Key Lock都有S锁和X锁两种，但Insert Intention Lock和隐式锁不区分S锁和X锁。

#### 隐式锁是如何工作的？

聚簇索引记录有个trx_id隐藏列，记录着最后修改记录的事务id，新插入一条记录后记录的trx_id就是insert事务的id，如果有其他事务想对该记录加Record Lock，首先会看一下记录的trx_id是否是当前活跃的事务，如果不是就可以正常加锁，否则就帮前面的insert事务生成一个X型的Record Lock，然后自己再生成一个Record Lock并进入等待状态。

二级索引没有trx_id隐藏列，但二级索引页面的Page Header部分有一个PAGE_MAX_TRX_ID属性，PAGE_MAX_TRX_ID记录着修改该页面的最大事务id，如果PAGE_MAX_TRX_ID小于当前最小的活跃事务id，那么表示对页面做修改的事务已经提交，可以直接对二级索引记录加锁，否则就需要回表之后再对聚簇索引加锁，重复上面的逻辑。

隐式锁起到了对insert的新记录延迟加锁的作用，减少了生成锁结构的开销。

#### 行锁在内存中的结构？

除了隐式锁之外，其他锁都需要在内存中生成一个锁结构，如果多个锁符合一下条件，那么可以把这些锁放在一个锁结构中，避免每行记录都创建一个锁结构，以减少创建锁结构的开销：

- 同一个事务中加的锁。
- 被加锁的记录在用一个页面中。
- 锁的类型是一样的，比如都是X型的Record Lock。
- 等待状态是一样的。

#### UPDATE和DELETE如何加锁？

锁定读、UPDATE、DELETE的加锁逻辑大致相同：

- 在不大于READ COMMITTED隔离级别下加的是Record Lock；而在不小于REPEATABLE READ隔离级别下加的是Next-Key Lock。如果访问方法是const（唯一性搜索），无论哪个隔离级别都是加Record Lock。
- 直接扫描聚簇索引的情况，在不大于READ COMMITTED隔离级别下，凡是满足where条件的记录都加锁，不满足的不加锁；在不小于REPEATABLE READ隔离级别下，只要是在扫描区间中被扫过的记录都会被加锁。注意，形成扫描区间的条件只是where条件的一部分，所以扫描区间的范围要比where条件大。
- 如果是先扫描二级索引再回表的情况，在不大于READ COMMITTED隔离级别下，凡是满足where条件的二级索引记录和聚簇索引记录都加锁，不满足的不加锁；在不小于REPEATABLE READ隔离级别下，只要是在扫描区间中被扫过的二级索引记录都会被加锁，但是如果不满足回表条件，那么就不会回表，也就不会对聚簇索引记录加锁，即只有满足where条件的聚簇索引记录才会被加锁。

还有很多其他特殊情况有着特殊的加锁逻辑，不过这种特殊情况一般出现在边界条件的记录上。

锁定读、UPDATE、DELETE的加锁逻辑也各自有独特的地方：

- 锁定读可以选择加S锁或者X锁，UPDATE、DELETE只能加X锁。
- UPDATE即使搜索时使用不到二级索引，但是可能更新的字段有索引，这个时候同样需要对二级索引加锁。
- UPDATE、DELETE对被更新或者删除的二级索引记录加的是隐式锁。

#### INSERT如何加锁？

INSERT如果遇到gap锁（gap锁或next-key锁）会使用插入意向锁；如果没有遇到一般会使用隐式锁，不过也有特殊情况：

- 遇到重复主键，会给被重复的记录加锁，不大于READ COMMITTED加的是S型Record Lock，不小于REPEATABLE READ加的是S型Next-Key Lock。
- 遇到外键检查，如果外键在父表中存在，直接给父表记录加S型Record Lock；如果在父表中不存在，此时插入虽然失败，但是如果隔离级别不小于REPEATABLE READ，则会给父表大于指定外键的记录加上gap锁。

#### 如何优化一条SQL？

待整理............

先想一下这条SQL该怎么执行最好，单表走什么索引，使用什么连接算法，走索引吗，子查询应该怎么执行，半连接还是物化？然后explain看一下符不符合预期。这个问题太复杂了，包含的内容太多了，真的不好回答。最多的就是看走没走索引，为什么没走索引，还需不需要再建索引，像pg我还可以vuccum一下表，调整fillfactor等参数，MySQL好像没什么参数可调？调整参数也可整理一下。然后就技术上的优化就没了，剩下的可以结合业务进行优化，比如表结构合不合理，某些字段是不是类型太大了，表是不是还可再规范化，满足三范式原则，等等。

使用系统变量optimizer_switch控制查询优化器，别使用hash join。

对比数据库的redo日志方案和log合并树的方案？











