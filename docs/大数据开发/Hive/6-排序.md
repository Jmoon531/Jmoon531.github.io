---

Created at: 2021-10-27
Last updated at: 2025-03-05
Source URL: https://yuanbao.tencent.com/chat/naQivTmsDa/c6983595-c89d-4537-a2f3-0cf9ae7e341a


---

# 6-排序


1.**order by** 全局有序，默认升序ASC，设置reduce个数无效，永远只有一个reduce，生产环境下不允许使用order by，会通过配置不执行通过order by排序的SQL，因为只有1个reduce，会造成数据倾斜，reduce任务过重。不过order by 和 limit 连用还是可以，比如说取top10，也就是降序排，然后limit 10，这时hive会开10个map任务，然后让每个map任务取top10，最后再在一个reduce里从100条数据里取top10。(MapReduce程序见末尾)

2.**sort by** 会对分区内的数据排序，分区的个数等于设置的reduce个数，如果不与distribute by 连用的话，分区的规则是随机地对数据分区。
在当前会话中设置reduce个数
```
set mapreduce.job.reduces=3;
```
查看reduce个数
```
set mapreduce.job.reduces;
```
根据部门编号降序查看员工信息，可以看到并不是全局有序的，而是部分有序的
```
select * from emp sort by deptno desc;
```
或者将查询结果导入到文件中，与MapReduce设置分区后一样，会保存到多个文件中，在每个文件里的数据是有序的
```
insert overwrite local directory '/opt/module/data/sortby-result'
select * from emp sort by deptno desc;
```

3.**distribute by** 指定按哪个字段进行分区，分区的个数等于设置的reduce个数，常与sort by 连用
先设置reduce个数大于1，不然看不到效果
```
set mapreduce.job.reduces=3;
```
比如只对查询结果分区：
```
insert overwrite local directory '/opt/module/data/distribute-result'
select * from emp
distribute by deptno
```
对查询结果分区后排序：
```
insert overwrite local directory '/opt/module/data/distribute-result'
select * from emp
distribute by deptno
sort by empno desc;
```
注意：

* distribute by 的分区规则是根据分区字段的 hash 码与 reduce 的个数进行模除后， 余数相同的分到一个区。
* Hive 要求 DISTRIBUTE BY 语句要写在 SORT BY 语句之前。

4 **cluster by**，当 distribute by 和 sorts by 字段相同时，可以使用 cluster by，但是cluster by不能指定排序规 ASC、DESC，默认只能是升序。也就是说 cluster by是distribute by 和 sorts by 字段相同且按升序规则排序时的简写规则。
以下两种写法等价：
```
select * from emp cluster by deptno;
select * from emp distribute by deptno sort by deptno;
```

**总结：**

|     |     |     |     |     |
| --- | --- | --- | --- | --- |
| 关键字 | 作用  | 是否全局有序 | Reducer 数量 | 排序方向限制 |
| **ORDER BY** | 全局排序 | 是   | 1   | 可指定 ASC/DESC |
| **SORT BY** | 局部排序（每个 Reducer 内部有序） | 否   | 多个  | 可指定 ASC/DESC |
| **DISTRIBUTE BY** | 按字段哈希分区 | 否   | 多个  | 无   |
| **CLUSTER BY** | 按字段哈希分区 + 组内升序排序 | 否   | 多个  | 仅升序 |

**MapReduce程序：求消费金额top10的订单**
输入数据：因为MapTask的数量由切片数量决定，所以如果想开启3个MapTask就需要将输入文件分成3个文件
Order类
```
public class Order implements WritableComparable<Order> {

    private String name;
    private String orderDate;
    private double cost;

    public Order() {
    }

    public Order(String name, String orderDate, double cost) {
        this.name = name;
        this.orderDate = orderDate;
        this.cost = cost;
    }

    @Override
    public void write(DataOutput out) throws IOException {
        out.writeUTF(name);
        out.writeUTF(orderDate);
        out.writeDouble(cost);
    }

    @Override
    public void readFields(DataInput in) throws IOException {
        name = in.readUTF();
        orderDate = in.readUTF();
        cost = in.readDouble();
    }

    @Override
    public int compareTo(Order o) {
        return Double.compare(o.cost, this.cost);
    }

    @Override
    public String toString() {
        return name + ',' + orderDate + ',' + cost;
    }

    public String getName() {
        return name;
    }

    public String getOrderDate() {
        return orderDate;
    }

    public double getCost() {
        return cost;
    }
}
```
map
```
public class TopNMapper extends Mapper<LongWritable, Text, DoubleWritable, Order> {

    DoubleWritable doubleWritable = new DoubleWritable();
    PriorityQueue<Order> queue = new PriorityQueue<>();

    @Override
    protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, DoubleWritable, Order>.Context context) throws IOException, InterruptedException {
        String[] split = value.toString().split(",");
        double cost = Double.parseDouble(split[2]);
        Order order = new Order(split[0], split[1], cost);
        queue.add(order);
    }

    @Override
    protected void cleanup(Mapper<LongWritable, Text, DoubleWritable, Order>.Context context) throws IOException, InterruptedException {
        for (int i = 0; i < 3; i++) {
            Order order = queue.remove();
            doubleWritable.set(order.getCost());
            context.write(doubleWritable, order);
        }
    }
}
```
reduce
```
public class TopNReducer extends Reducer<DoubleWritable, Order, NullWritable, Order> {

    PriorityQueue<Order> queue = new PriorityQueue<>();

    @Override
    protected void reduce(DoubleWritable key, Iterable<Order> values, Reducer<DoubleWritable, Order, NullWritable, Order>.Context context) throws IOException, InterruptedException {
        for (Order value : values) {
            Order order = new Order(value.getName(), value.getOrderDate(), value.getCost());
            queue.add(order);
        }
    }

    @Override
    protected void cleanup(Reducer<DoubleWritable, Order, NullWritable, Order>.Context context) throws IOException, InterruptedException {
        for (int i = 0; i < 3; i++) {
            context.write(NullWritable.get(), queue.remove());
        }
    }
}
```
Driver
```
public class TopNDriver {
    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
        Job job = Job.getInstance(new Configuration());
        job.setJarByClass(TopNDriver.class);
        job.setMapperClass(TopNMapper.class);
        job.setReducerClass(TopNReducer.class);
        job.setMapOutputKeyClass(DoubleWritable.class);
        job.setMapOutputValueClass(Order.class);
        job.setOutputKeyClass(NullWritable.class);
        job.setOutputValueClass(Order.class);
        String classPath = Thread.currentThread().getContextClassLoader().getResource("").getPath();
        FileInputFormat.setInputPaths(job, new Path(classPath + "/input/topN"));
        FileOutputFormat.setOutputPath(job, new Path(classPath + "/top3"));
        boolean b = job.waitForCompletion(true);
        System.exit(b ? 0 : 1);
    }
}
```

