---

Created at: 2021-09-04
Last updated at: 2021-10-27
Source URL: about:blank


---

# 13-用户自定义函数


当 Hive 提供的内置函数无法满足你的业务处理需要时，此时就需要用户自定义函数了，自定义函数的步骤：
（1）继承 Hive 提供的类
org.apache.hadoop.hive.ql.udf.generic.GenericUDF
或 org.apache.hadoop.hive.ql.udf.generic.GenericUDTF
或 org.apache.hadoop.hive.ql.udf.generic.GenericUDAF
（2）实现类中的抽象方法
（3）添加 jar（或者直接拖到Hive的lib目录下，就不需要指定 jar包的具体路径了），可以上传到本地文件系统，也可以上传到HDFS上
add jar linux\_jar\_path
（4）在 hive 的命令行窗口创建函数
create \[temporary\] function \[dbname.\]function\_name AS 'class\_name';
不要了，也可以在 hive 的命令行窗口删除函数
drop \[temporary\] function \[if exists\] \[dbname.\]function\_name;
注意创建函数需要指定数据库名，和表一样Hive的自定义函数属于一个数据库，不指定默认是当前库。
（5）第3步和第4步可以合并到一起
create \[temporary\] function \[dbname.\]function\_name as class\_name using jar 'jar\_path';

例如自定义UDTF函数炸裂JSON数组：
1.继承org.apache.hadoop.hive.ql.udf.generic.GenericUDAF，并实现 initialize、process、close方法。initialize在只在开始时调用一次，用于检查UDTF函数输入参数的个数和类型的合法性，以及声明UDTF函数输出结果的所有的列的 列名 和 列的类型。process方法会处理输入的每一行数据，有多少行数据就会调用多少次。close在所有行处理完后调用。
```
public class ExplodeJSONArray extends GenericUDTF {

    private PrimitiveObjectInspector inputOI;

    @Override
    public void close() throws HiveException {}

    /**
     * argOIs是UDTF函数输入参数的对象检查器
     * 比如 udtf_explode(col, ",") ，那么自定义UDTF函数udtf_explode有两个输入参数，
     * 于是argOIs数组的长度为2，argOIs[0]是col列的数据类型的ObjectInspector，argOIs[1]是","的ObjectInspector，
     * 返回值 StructObjectInspector 是自定义UDTF函数返回的列的 列名 和 数据类型，因为UDTF函数可以有多个列，
     * 所以 列名 和 列的数据类型 都是放在List集合中的。
     */
    @Override
    public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
        if(argOIs.length!=1){
            throw new UDFArgumentException("explode_json_array函数只能接收1个参数");
        }
        ObjectInspector argOI = argOIs[0];
        if(argOI.getCategory()!=ObjectInspector.Category.PRIMITIVE){
            throw new UDFArgumentException("explode_json_array函数只能接收基本数据类型的参数");
        }
        PrimitiveObjectInspector primitiveOI  = (PrimitiveObjectInspector) argOI;
        inputOI=primitiveOI;
        if(primitiveOI.getPrimitiveCategory()!=PrimitiveObjectInspector.PrimitiveCategory.STRING){
            throw new UDFArgumentException("explode_json_array函数只能接收STRING类型的参数");
        }
        ArrayList<String> fieldNames = new ArrayList<>();
        ArrayList<ObjectInspector> fieldOIs = new ArrayList<>();
        fieldNames.add("item");
        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
    }

    /**
     * args是UDTF函数的输入参数，
     * 比如 udtf_explode(col, ",") ，那么args[0]是col列的数据，args[1]是","。
     * forward传递的参数是UDTF函数的结果中的一行数据，因为UDTF函数的结果可以是多个列，
     * 所以传递的必须是一个数组或者List集合。
     */
    @Override
    public void process(Object[] args) throws HiveException {
        Object arg = args[0];
        String jsonArrayStr = PrimitiveObjectInspectorUtils.getString(arg, inputOI);
        //hive内置的JSONArray
        JSONArray jsonArray = new JSONArray(jsonArrayStr);
        for (int i = 0; i < jsonArray.length(); i++) {
            String json = jsonArray.getString(i);
            String[] result = {json};
            forward(result);
        }
    }
}
```

2.添加 jar
```
add jar /opt/software/Hive-1.0-SNAPSHOT.jar;
```
或者：
```
add jar hdfs://hadoop102:8020/user/hive/jars/Hive-1.0-SNAPSHOT.jar;
```

3.在 hive 的命令行窗口创建函数
```
create temporary function explodeJSONArray AS "CustomUTDF.ExplodeJSONArray";
```

第2、3步合并的操作命令：
```
create temporary function explodeJSONArray AS "CustomUTDF.ExplodeJSONArray" using jar 'hdfs://hadoop102:8020/user/hive/jars/Hive-1.0-SNAPSHOT.jar';
```

查看是否创建成功：
```
show functions like '*json*';
```

4.创建表
```
create table json(line string);
```
5.导入数据
```
[{"name": "zhangsan","age": 18},{"name": "lisi","age": 20}]
```
6.使用自定义UDTF函数
```
select explodeJSONArray(line) from json;
```
结果：
```
item
{"name":"zhangsan","age":18}
{"name":"lisi","age":20}
```

