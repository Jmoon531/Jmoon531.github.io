---

Created at: 2021-09-26
Last updated at: 2021-09-29


---

# 14-闭包与序列化


在 Scala 的函数式编程中，闭包是匿名函数的常规操作，所以经常会出现RDD算子内的匿名函数用到算子外的数据，这也就形成了闭包的效果，但RDD算子内的匿名函数不仅会闭包变量，还要求被闭包的变量能被序列化，因为算子以外的代码都是在 Driver 端执行，但算子里面的代码都是在 Executor端执行的，所以如果使用的算子外的数据无法序列化，就意味着无法传值给 Executor端执行，于是就会发生错误，Spark需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。

案例一：
```
def main(args: Array[String]): Unit = {
  val sc = new SparkContext("local[*]", "sortByKey")
  val rdd = sc.makeRDD(List[Int](1, 2, 3, 4))
  val user = new User
 `// RDD算子中传递的匿名函数会进行闭包检测，如果闭包了算子外的数据，并且该无法被序列化，`
 `// 那么就意味着无法传值给 Executor端执行，于是就会抛出异常`
  rdd.foreach(
    num => {
      println("age = " + (user.age + num))
    }
  )
 `println(user)  //User(age=30)`
  sc.stop()
}
`//  class User {`
`//  class User extends Serializable {`
`// 样例类在编译时，会自动混入序列化特质（实现可序列化接口）`
case class User() {
  var age: Int = 30
  override def toString = s"User(age=$age)"
}
```
实验现象及结论：
1.如果直接class User，那么会抛出序列化异常，如果class User extends Serializable 或者 case class User() 时才可将数据传到Executor执行。
2.println(user)是在Driver 端执行的，所以输出结果是User(age=30)，并且user被输出传到Executor后会在各个Executor内存反序列化成新的对象，这些对象之间是独立，没有关联。

案例二：
```
def main(args: Array[String]): Unit = {
  val sc = new SparkContext("local[*]", "Serializable")
  val rdd: RDD[String] = sc.makeRDD(Array("hello world", "hello spark", "hive", "atguigu"))
  //匹配包含h的数据
  val search = new Search("h")

  //序列化异常
  search.getMatch1(rdd).collect().foreach(println)

  //同样会序列化异常，因为search作为一个变量出现在函数，所以闭包检测时会要求类能够被序列化
  //rdd.filter(x => x.contains(search.query)).collect().foreach(println)

  //可以正常执行
  //search.getMatch2(rdd).collect().foreach(println)

  sc.stop()
}

// 类的构造参数其实是类的属性
class Search(val query: String) {

  `// 不管是不是匿名函数，函数作为参数传递都会进行闭包，`
 `// 所以isMatch作为参数传递给rdd.filter后同样需要进行闭包检测，`
 `def isMatch(x: String): Boolean = {`
 `// 传递给RDD算子的函数不仅会闭包变量，还要求被闭包的变量能被序列化，`
 `// 因为类的对象作为一个变量出现在函数，所以闭包检测时会要求类能够被序列化`
 `x.contains(this.query)`
  }

  def getMatch1(rdd: RDD[String]): RDD[String] = {
    //将isMatch作为参数传递给rdd.filter
    rdd.filter(isMatch)
  }

  def getMatch2(rdd: RDD[String]): RDD[String] = {
    `// 局部变量可以序列化，不会抛序列化异常`
 `val s = query`
 `rdd.filter(x => x.contains(s))`
  }
}
```
实验结论：
1.不管是不是匿名函数，函数作为参数传递都会进行闭包
2.传递给RDD算子的函数不仅会闭包变量，还要求被闭包的变量能被序列化
3.类的对象作为一个变量出现在被传递给RDD算子的函数中，所以闭包检测时会要求类能够被序列化

Kryo 序列化框架
Java 的序列化能够序列化任何的类。但是比较重（字节多） ，序列化后，对象的提交也比较大。 Spark 出于性能的考虑， Spark2.0 开始支持另外一种 Kryo 序列化机制。 Kryo 速度是 Serializable 的 10 倍。当 RDD 在 Shuffle 数据的时候，简单数据类型、数组和字符串类型已经在 Spark 内部使用 Kryo 来序列化，其它类型没有使用Kryo进行序列化，如自定义类就没有，而是使用的Java的序列机制，当然也可以将自定义类设置为使用Kryo 序列化。 使用 Kryo 序列化，也要继承 Serializable 接口，但是transient关键字就并不起作用了。

