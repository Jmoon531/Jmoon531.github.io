<!doctype html><html lang=zh class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content=江岳亮的博客><meta name=author content=Jmoon><link href=../../Scala/10-Scala%E7%9A%84%E6%96%B9%E6%B3%95%20%E4%B8%8E%20Java%E7%9A%84%E6%96%B9%E6%B3%95%E5%BC%95%E7%94%A8/ rel=prev><link href=../2-Spark%20%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%20%E5%92%8C%20Yarn%20Client%20%E6%A8%A1%E5%BC%8F/ rel=next><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.10"><title>1-Spark快速上手 - Jmoon's Blog</title><link rel=stylesheet href=../../../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CSource+Code+Pro:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Source Code Pro"}</style><link rel=stylesheet href=/assets/stylesheets/font.css><link rel=stylesheet href=/assets/stylesheets/custom.css><link rel=stylesheet href=https://unpkg.com/katex@0/dist/katex.min.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><link href=../../../assets/stylesheets/glightbox.min.css rel=stylesheet><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style><script src=../../../assets/javascripts/glightbox.min.js></script></head> <body dir=ltr data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=light> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#1-spark class=md-skip> 跳转至 </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=页眉> <a href=../../.. title="Jmoon's Blog" class="md-header__button md-logo" aria-label="Jmoon's Blog" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Jmoon's Blog </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 1-Spark快速上手 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=light aria-label=切换到浅色模式 type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title=切换到浅色模式 for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=white data-md-color-accent=dark aria-label=切换到黑暗模式 type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title=切换到黑暗模式 for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=搜索 placeholder=搜索 autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=查找> <a href=javascript:void(0) class="md-search__icon md-icon" title=分享 aria-label=分享 data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=清空当前内容 aria-label=清空当前内容 tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 正在初始化搜索引擎 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=标签 data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> home </a> </li> <li class=md-tabs__item> <a href=../../../Java%E5%9F%BA%E7%A1%80/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/1-IO/ class=md-tabs__link> Java基础 </a> </li> <li class=md-tabs__item> <a href=../../../Web%E5%BC%80%E5%8F%91/MyBatis/1.MyBatis%E5%85%A5%E9%97%A8-%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5/ class=md-tabs__link> Web开发 </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../Flink/0-Flink%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE/ class=md-tabs__link> 大数据开发 </a> </li> <li class=md-tabs__item> <a href=../../../%E9%97%AE%E7%AD%94/Flink/ class=md-tabs__link> 问答 </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=导航栏 data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="Jmoon's Blog" class="md-nav__button md-logo" aria-label="Jmoon's Blog" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> Jmoon's Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> home </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../Java%E5%9F%BA%E7%A1%80/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/1-IO/ class=md-nav__link> <span class=md-ellipsis> Java基础 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../Web%E5%BC%80%E5%8F%91/MyBatis/1.MyBatis%E5%85%A5%E9%97%A8-%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5/ class=md-nav__link> <span class=md-ellipsis> Web开发 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4 checked> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex> <span class=md-ellipsis> 大数据开发 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=true> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> 大数据开发 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Flink/0-Flink%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE/ class=md-nav__link> <span class=md-ellipsis> Flink </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Hadoop/1-Hadoop%E7%9A%84%E7%90%86%E8%A7%A3/ class=md-nav__link> <span class=md-ellipsis> Hadoop </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../HBase/1-HBase%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/ class=md-nav__link> <span class=md-ellipsis> HBase </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Hive/1-Hive%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/ class=md-nav__link> <span class=md-ellipsis> Hive </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Kafka/1-Kafka%E6%9E%B6%E6%9E%84%E5%8F%8A%E7%90%86%E8%A7%A3/ class=md-nav__link> <span class=md-ellipsis> Kafka </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../Scala/1-Hello%20Scala/ class=md-nav__link> <span class=md-ellipsis> Scala </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_7 checked> <label class=md-nav__link for=__nav_4_7 id=__nav_4_7_label tabindex=0> <span class=md-ellipsis> Spark </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_7_label aria-expanded=true> <label class=md-nav__title for=__nav_4_7> <span class="md-nav__icon md-icon"></span> Spark </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 1-Spark快速上手 </span> </a> </li> <li class=md-nav__item> <a href=../2-Spark%20%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%20%E5%92%8C%20Yarn%20Client%20%E6%A8%A1%E5%BC%8F/ class=md-nav__link> <span class=md-ellipsis> 2-Spark 核心组件 和 Yarn Client 模式 </span> </a> </li> <li class=md-nav__item> <a href=../3-RDD%E7%9A%84%E5%88%9B%E5%BB%BA/ class=md-nav__link> <span class=md-ellipsis> 3-RDD的创建 </span> </a> </li> <li class=md-nav__item> <a href=../4-RDD%E5%B9%B6%E8%A1%8C%E5%BA%A6%E4%B8%8E%E5%88%86%E5%8C%BA/ class=md-nav__link> <span class=md-ellipsis> 4-RDD并行度与分区 </span> </a> </li> <li class=md-nav__item> <a href=../5-RDD%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90%20%E4%B9%8B%20Value%20%E7%B1%BB%E5%9E%8B/ class=md-nav__link> <span class=md-ellipsis> 5-RDD转换算子 之 Value 类型 </span> </a> </li> <li class=md-nav__item> <a href=../6-RDD%20Value%20%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90%20%E4%B9%8B%20groupBy/ class=md-nav__link> <span class=md-ellipsis> 6-RDD Value 类型转换算子 之 groupBy </span> </a> </li> <li class=md-nav__item> <a href=../7-Spark%E7%9A%84%E7%90%86%E8%A7%A3/ class=md-nav__link> <span class=md-ellipsis> 7-Spark的理解 </span> </a> </li> <li class=md-nav__item> <a href=../8-%E5%90%91Yarn%E9%9B%86%E7%BE%A4%E6%8F%90%E4%BA%A4Spark%E5%BA%94%E7%94%A8/ class=md-nav__link> <span class=md-ellipsis> 8-向Yarn集群提交Spark应用 </span> </a> </li> <li class=md-nav__item> <a href=../9-RDD%E7%9A%84%20%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB%E3%80%81Shuffle%E3%80%81%E9%98%B6%E6%AE%B5%E5%88%92%E5%88%86%E3%80%81%E4%BB%BB%E5%8A%A1%E5%88%92%E5%88%86/ class=md-nav__link> <span class=md-ellipsis> 9-RDD的 依赖关系、Shuffle、阶段划分、任务划分 </span> </a> </li> <li class=md-nav__item> <a href=../10-RDD%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90%20%E4%B9%8B%20Value%20%E7%B1%BB%E5%9E%8B%EF%BC%88%E7%BB%AD%EF%BC%89/ class=md-nav__link> <span class=md-ellipsis> 10-RDD转换算子 之 Value 类型（续） </span> </a> </li> <li class=md-nav__item> <a href=../11-RDD%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90%20%E4%B9%8B%20%E5%8F%8Cvalue%E7%B1%BB%E5%9E%8B/ class=md-nav__link> <span class=md-ellipsis> 11-RDD转换算子 之 双value类型 </span> </a> </li> <li class=md-nav__item> <a href=../12-RDD%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90%20%E4%B9%8B%20Key-Value%20%E7%B1%BB%E5%9E%8B/ class=md-nav__link> <span class=md-ellipsis> 12-RDD转换算子 之 Key-Value 类型 </span> </a> </li> <li class=md-nav__item> <a href=../13-RDD%E8%A1%8C%E5%8A%A8%E7%AE%97%E5%AD%90/ class=md-nav__link> <span class=md-ellipsis> 13-RDD行动算子 </span> </a> </li> <li class=md-nav__item> <a href=../14-%E9%97%AD%E5%8C%85%E4%B8%8E%E5%BA%8F%E5%88%97%E5%8C%96/ class=md-nav__link> <span class=md-ellipsis> 14-闭包与序列化 </span> </a> </li> <li class=md-nav__item> <a href=../15-%E6%8C%81%E4%B9%85%E5%8C%96/ class=md-nav__link> <span class=md-ellipsis> 15-持久化 </span> </a> </li> <li class=md-nav__item> <a href=../16-%E7%B4%AF%E5%8A%A0%E5%99%A8/ class=md-nav__link> <span class=md-ellipsis> 16-累加器 </span> </a> </li> <li class=md-nav__item> <a href=../17-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/ class=md-nav__link> <span class=md-ellipsis> 17-广播变量 </span> </a> </li> <li class=md-nav__item> <a href=../18-SparkCore%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D/ class=md-nav__link> <span class=md-ellipsis> 18-SparkCore案例实操 </span> </a> </li> <li class=md-nav__item> <a href=../20-SparkSQL%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%B8%AD%E4%BD%BF%E7%94%A8/ class=md-nav__link> <span class=md-ellipsis> 20-SparkSQL命令行中使用 </span> </a> </li> <li class=md-nav__item> <a href=../21-SparkSQL%20IDEA%E4%B8%AD%E4%BD%BF%E7%94%A8/ class=md-nav__link> <span class=md-ellipsis> 21-SparkSQL IDEA中使用 </span> </a> </li> <li class=md-nav__item> <a href=../22-SparkSQL%E7%94%A8%E6%88%B7%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0/ class=md-nav__link> <span class=md-ellipsis> 22-SparkSQL用户自定义函数 </span> </a> </li> <li class=md-nav__item> <a href=../23-SparkSQL%20%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BF%9D%E5%AD%98/ class=md-nav__link> <span class=md-ellipsis> 23-SparkSQL 数据的加载和保存 </span> </a> </li> <li class=md-nav__item> <a href=../24-SparkSQL%20%E8%BF%9E%E6%8E%A5%20Hive/ class=md-nav__link> <span class=md-ellipsis> 24-SparkSQL 连接 Hive </span> </a> </li> <li class=md-nav__item> <a href=../26-SparkStreaming%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%E7%9A%84%E6%96%B9%E5%BC%8F/ class=md-nav__link> <span class=md-ellipsis> 26-SparkStreaming获取数据的方式 </span> </a> </li> <li class=md-nav__item> <a href=../27-SparkStreaming%E5%8E%9F%E8%AF%AD/ class=md-nav__link> <span class=md-ellipsis> 27-SparkStreaming原语 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../ZooKeeper/1-Zookeeper%20%E7%9A%84%E7%90%86%E8%A7%A3/ class=md-nav__link> <span class=md-ellipsis> ZooKeeper </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/ class=md-nav__link> <span class=md-ellipsis> 数据仓库 </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../%E9%97%AE%E7%AD%94/Flink/ class=md-nav__link> <span class=md-ellipsis> 问答 </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=1-spark>1-Spark快速上手<a class=headerlink href=#1-spark title="Permanent link">#</a></h1> <div style="margin-top: -30px; font-size: 0.75em; opacity: 0.7;"> <p><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10h-2a8 8 0 0 1-8 8 8 8 0 0 1-8-8 8 8 0 0 1 8-8zm6.78 1a.7.7 0 0 0-.48.2l-1.22 1.21 2.5 2.5L20.8 5.7c.26-.26.26-.7 0-.95L19.25 3.2c-.13-.13-.3-.2-.47-.2m-2.41 2.12L9 12.5V15h2.5l7.37-7.38z"/></svg></span> 约 3530 个字 <span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 17H7V3h14m0-2H7a2 2 0 0 0-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V3a2 2 0 0 0-2-2M3 5H1v16a2 2 0 0 0 2 2h16v-2H3m12.96-10.71-2.75 3.54-1.96-2.36L8.5 15h11z"/></svg></span> 3 张图片 <span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 20c4.42 0 8-3.58 8-8s-3.58-8-8-8-8 3.58-8 8 3.58 8 8 8m0-18c5.5 0 10 4.5 10 10s-4.5 10-10 10C6.47 22 2 17.5 2 12S6.5 2 12 2m.5 11H11V7h1.5v4.26l3.7-2.13.75 1.3z"/></svg></span> 预计阅读时间 71 分钟</p> </div> <p><strong>IDEA本地模式跑Spark程序</strong> 1.配置好Scala的环境 2.导入Spark的依赖 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1></a><a href=#__codelineno-0-1><span class=linenos data-linenos="1 "></span></a>&lt;dependency&gt;
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2></a><a href=#__codelineno-0-2><span class=linenos data-linenos="2 "></span></a>    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3></a><a href=#__codelineno-0-3><span class=linenos data-linenos="3 "></span></a>    &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt;
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4></a><a href=#__codelineno-0-4><span class=linenos data-linenos="4 "></span></a>    &lt;version&gt;3.0.0&lt;/version&gt;
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5></a><a href=#__codelineno-0-5><span class=linenos data-linenos="5 "></span></a>&lt;/dependency&gt;
</span></code></pre></div> 3.添加log4j.properties <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1></a><a href=#__codelineno-1-1><span class=linenos data-linenos=" 1 "></span></a>log4j.rootCategory=ERROR, console
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2></a><a href=#__codelineno-1-2><span class=linenos data-linenos=" 2 "></span></a>log4j.appender.console=org.apache.log4j.ConsoleAppender
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3></a><a href=#__codelineno-1-3><span class=linenos data-linenos=" 3 "></span></a>log4j.appender.console.target=System.err
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4></a><a href=#__codelineno-1-4><span class=linenos data-linenos=" 4 "></span></a>log4j.appender.console.layout=org.apache.log4j.PatternLayout
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5></a><a href=#__codelineno-1-5><span class=linenos data-linenos=" 5 "></span></a>log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6></a><a href=#__codelineno-1-6><span class=linenos data-linenos=" 6 "></span></a>
</span><span id=__span-1-7><a id=__codelineno-1-7 name=__codelineno-1-7></a><a href=#__codelineno-1-7><span class=linenos data-linenos=" 7 "></span></a># Set the default spark-shell log level to ERROR. When running the spark-shell, the
</span><span id=__span-1-8><a id=__codelineno-1-8 name=__codelineno-1-8></a><a href=#__codelineno-1-8><span class=linenos data-linenos=" 8 "></span></a># log level for this class is used to overwrite the root logger&#39;s log level, so that
</span><span id=__span-1-9><a id=__codelineno-1-9 name=__codelineno-1-9></a><a href=#__codelineno-1-9><span class=linenos data-linenos=" 9 "></span></a># the user can have different defaults for the shell and regular Spark apps.
</span><span id=__span-1-10><a id=__codelineno-1-10 name=__codelineno-1-10></a><a href=#__codelineno-1-10><span class=linenos data-linenos="10 "></span></a>log4j.logger.org.apache.spark.repl.Main=ERROR
</span><span id=__span-1-11><a id=__codelineno-1-11 name=__codelineno-1-11></a><a href=#__codelineno-1-11><span class=linenos data-linenos="11 "></span></a>
</span><span id=__span-1-12><a id=__codelineno-1-12 name=__codelineno-1-12></a><a href=#__codelineno-1-12><span class=linenos data-linenos="12 "></span></a># Settings to quiet third party logs that are too verbose
</span><span id=__span-1-13><a id=__codelineno-1-13 name=__codelineno-1-13></a><a href=#__codelineno-1-13><span class=linenos data-linenos="13 "></span></a>log4j.logger.org.spark_project.jetty=ERROR
</span><span id=__span-1-14><a id=__codelineno-1-14 name=__codelineno-1-14></a><a href=#__codelineno-1-14><span class=linenos data-linenos="14 "></span></a>log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR
</span><span id=__span-1-15><a id=__codelineno-1-15 name=__codelineno-1-15></a><a href=#__codelineno-1-15><span class=linenos data-linenos="15 "></span></a>log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=ERROR
</span><span id=__span-1-16><a id=__codelineno-1-16 name=__codelineno-1-16></a><a href=#__codelineno-1-16><span class=linenos data-linenos="16 "></span></a>log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=ERROR
</span><span id=__span-1-17><a id=__codelineno-1-17 name=__codelineno-1-17></a><a href=#__codelineno-1-17><span class=linenos data-linenos="17 "></span></a>log4j.logger.org.apache.parquet=ERROR
</span><span id=__span-1-18><a id=__codelineno-1-18 name=__codelineno-1-18></a><a href=#__codelineno-1-18><span class=linenos data-linenos="18 "></span></a>log4j.logger.parquet=ERROR
</span><span id=__span-1-19><a id=__codelineno-1-19 name=__codelineno-1-19></a><a href=#__codelineno-1-19><span class=linenos data-linenos="19 "></span></a>
</span><span id=__span-1-20><a id=__codelineno-1-20 name=__codelineno-1-20></a><a href=#__codelineno-1-20><span class=linenos data-linenos="20 "></span></a># SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support
</span><span id=__span-1-21><a id=__codelineno-1-21 name=__codelineno-1-21></a><a href=#__codelineno-1-21><span class=linenos data-linenos="21 "></span></a>log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
</span><span id=__span-1-22><a id=__codelineno-1-22 name=__codelineno-1-22></a><a href=#__codelineno-1-22><span class=linenos data-linenos="22 "></span></a>log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR
</span></code></pre></div></p> <p><strong>WordCount案例</strong> 对RDD的操作和Scala的集合一样，只不过对Scala集合的每一步操作都是有结果的，即每一步操作都是会立即对集合进行计算，而RDD操作不是，在RDD的collect函数之前调用的RDD函数都只是一种声明，并不没有真正的对数据进行处理，只有执行到collect函数时才开始从文件中读取数据，并按之前声明好的步骤进行计算（与tensorflow一模一样，都是先构建计算图，构建完之后等到最后一步调用才开始计算，最后一步调用相当于是一个开关，按下去之后数据才开始流动）。RDD是通过装饰者模式实现这样的效果的，类似于Java IO的设计，RDD的每一步操作其实都是在创建对象，即将上一个对象包装到下一个对象中。 setMaster("local[*]")表示使用Local模式，其中的[*]表示使用与本地cpu核心数相同多的线程进行并行计算，也可以指定具体指定使用多少个线程，比如setMaster("local[2]")，如果不指定setMaster("local")则使用单线程。 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1></a><a href=#__codelineno-2-1><span class=linenos data-linenos=" 1 "></span></a>object WordCount {
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2></a><a href=#__codelineno-2-2><span class=linenos data-linenos=" 2 "></span></a> def main(args: Array[String]): Unit = {
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3></a><a href=#__codelineno-2-3><span class=linenos data-linenos=" 3 "></span></a>   // TODO 建立和Spark框架的连接
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4></a><a href=#__codelineno-2-4><span class=linenos data-linenos=" 4 "></span></a>   val sparConf = new SparkConf().`setMaster(&quot;local[*]&quot;)`.setAppName(&quot;WordCount&quot;)
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5></a><a href=#__codelineno-2-5><span class=linenos data-linenos=" 5 "></span></a>   val sc = new SparkContext(sparConf)
</span><span id=__span-2-6><a id=__codelineno-2-6 name=__codelineno-2-6></a><a href=#__codelineno-2-6><span class=linenos data-linenos=" 6 "></span></a>   // TODO 执行业务操作
</span><span id=__span-2-7><a id=__codelineno-2-7 name=__codelineno-2-7></a><a href=#__codelineno-2-7><span class=linenos data-linenos=" 7 "></span></a>   // 1. 读取文件，获取一行一行的数据
</span><span id=__span-2-8><a id=__codelineno-2-8 name=__codelineno-2-8></a><a href=#__codelineno-2-8><span class=linenos data-linenos=" 8 "></span></a>   val lines: RDD[String] = sc.textFile(&quot;data&quot;)
</span><span id=__span-2-9><a id=__codelineno-2-9 name=__codelineno-2-9></a><a href=#__codelineno-2-9><span class=linenos data-linenos=" 9 "></span></a>   // 2. 映射：将每一行数据进行拆分，形成一个一个的单词（分词）放到一个List中
</span><span id=__span-2-10><a id=__codelineno-2-10 name=__codelineno-2-10></a><a href=#__codelineno-2-10><span class=linenos data-linenos="10 "></span></a>   //    扁平化：将所有List拆分成个体的操作
</span><span id=__span-2-11><a id=__codelineno-2-11 name=__codelineno-2-11></a><a href=#__codelineno-2-11><span class=linenos data-linenos="11 "></span></a>   val words: RDD[String] = lines.flatMap(_.split(&quot; &quot;))
</span><span id=__span-2-12><a id=__codelineno-2-12 name=__codelineno-2-12></a><a href=#__codelineno-2-12><span class=linenos data-linenos="12 "></span></a>   // 3. 将数据根据单词进行分组，便于统计，分组的结果是一个Map，key是分组的key，value是相同key的数据的List
</span><span id=__span-2-13><a id=__codelineno-2-13 name=__codelineno-2-13></a><a href=#__codelineno-2-13><span class=linenos data-linenos="13 "></span></a>   //    Map((Hello -&gt; List(Hello, Hello, Hello)), (world -&gt; List(world, world)))
</span><span id=__span-2-14><a id=__codelineno-2-14 name=__codelineno-2-14></a><a href=#__codelineno-2-14><span class=linenos data-linenos="14 "></span></a>   val wordGroup: RDD[(String, Iterable[String])] = words.groupBy(word =&gt; word)
</span><span id=__span-2-15><a id=__codelineno-2-15 name=__codelineno-2-15></a><a href=#__codelineno-2-15><span class=linenos data-linenos="15 "></span></a>   // 4. 对分组后的数据再进行map映射
</span><span id=__span-2-16><a id=__codelineno-2-16 name=__codelineno-2-16></a><a href=#__codelineno-2-16><span class=linenos data-linenos="16 "></span></a>   //    Map((hello -&gt; 3), (world -&gt; 2))
</span><span id=__span-2-17><a id=__codelineno-2-17 name=__codelineno-2-17></a><a href=#__codelineno-2-17><span class=linenos data-linenos="17 "></span></a>   val wordToCount: RDD[(String, Int)] = wordGroup.mapValues(_.size)
</span><span id=__span-2-18><a id=__codelineno-2-18 name=__codelineno-2-18></a><a href=#__codelineno-2-18><span class=linenos data-linenos="18 "></span></a>   // 5. 当调用collect时才会进行数据的计算。也就是说上面的步骤只是在声明操作，并没有真正的计算，只有执行collect方法时数据才会开始流动进行上面的操作
</span><span id=__span-2-19><a id=__codelineno-2-19 name=__codelineno-2-19></a><a href=#__codelineno-2-19><span class=linenos data-linenos="19 "></span></a>   // collect的返回值是Array，也就是最后会把RDD转换成Scala的数据结构，方便输出结果
</span><span id=__span-2-20><a id=__codelineno-2-20 name=__codelineno-2-20></a><a href=#__codelineno-2-20><span class=linenos data-linenos="20 "></span></a>   // Array((hello -&gt; 3), (world -&gt; 2))
</span><span id=__span-2-21><a id=__codelineno-2-21 name=__codelineno-2-21></a><a href=#__codelineno-2-21><span class=linenos data-linenos="21 "></span></a>   val array: Array[(String, Int)] = wordToCount.collect()
</span><span id=__span-2-22><a id=__codelineno-2-22 name=__codelineno-2-22></a><a href=#__codelineno-2-22><span class=linenos data-linenos="22 "></span></a>   array.foreach(println)
</span><span id=__span-2-23><a id=__codelineno-2-23 name=__codelineno-2-23></a><a href=#__codelineno-2-23><span class=linenos data-linenos="23 "></span></a>   // TODO 关闭连接
</span><span id=__span-2-24><a id=__codelineno-2-24 name=__codelineno-2-24></a><a href=#__codelineno-2-24><span class=linenos data-linenos="24 "></span></a>   sc.stop()
</span><span id=__span-2-25><a id=__codelineno-2-25 name=__codelineno-2-25></a><a href=#__codelineno-2-25><span class=linenos data-linenos="25 "></span></a> }
</span><span id=__span-2-26><a id=__codelineno-2-26 name=__codelineno-2-26></a><a href=#__codelineno-2-26><span class=linenos data-linenos="26 "></span></a>}
</span></code></pre></div> 还可以直接使用RDD提供的reduceByKey函数直接进行聚合操作，也就是将上面的第3、4步合并为一步，不用先分组然后再对每一组数据做map映射或者reduce聚合，不过需要注意的是，reduceByKey操作要求输入的数据必须是一个二元组，即kv键值对形式的，因为它的计算过程是直接对所有key相同的二元组的value做reduce操作。 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1></a><a href=#__codelineno-3-1><span class=linenos data-linenos=" 1 "></span></a>object WordCount2 {
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2></a><a href=#__codelineno-3-2><span class=linenos data-linenos=" 2 "></span></a>  def main(args: Array[String]): Unit = {
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3></a><a href=#__codelineno-3-3><span class=linenos data-linenos=" 3 "></span></a>    // TODO 建立和Spark框架的连接
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4></a><a href=#__codelineno-3-4><span class=linenos data-linenos=" 4 "></span></a>    val sparConf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;WordCount&quot;)
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5></a><a href=#__codelineno-3-5><span class=linenos data-linenos=" 5 "></span></a>    val sc = new SparkContext(sparConf)
</span><span id=__span-3-6><a id=__codelineno-3-6 name=__codelineno-3-6></a><a href=#__codelineno-3-6><span class=linenos data-linenos=" 6 "></span></a>    // TODO 执行业务操作
</span><span id=__span-3-7><a id=__codelineno-3-7 name=__codelineno-3-7></a><a href=#__codelineno-3-7><span class=linenos data-linenos=" 7 "></span></a>    // 1. 读取文件，获取一行一行的数据
</span><span id=__span-3-8><a id=__codelineno-3-8 name=__codelineno-3-8></a><a href=#__codelineno-3-8><span class=linenos data-linenos=" 8 "></span></a>    val lines: RDD[String] = sc.textFile(&quot;data&quot;)
</span><span id=__span-3-9><a id=__codelineno-3-9 name=__codelineno-3-9></a><a href=#__codelineno-3-9><span class=linenos data-linenos=" 9 "></span></a>    // 2. 映射：将每一行数据进行拆分，形成一个一个的单词（分词）放到一个List中
</span><span id=__span-3-10><a id=__codelineno-3-10 name=__codelineno-3-10></a><a href=#__codelineno-3-10><span class=linenos data-linenos="10 "></span></a>    //    扁平化：将所有List拆分成个体的操作
</span><span id=__span-3-11><a id=__codelineno-3-11 name=__codelineno-3-11></a><a href=#__codelineno-3-11><span class=linenos data-linenos="11 "></span></a>    val words: RDD[String] = lines.flatMap(_.split(&quot; &quot;))
</span><span id=__span-3-12><a id=__codelineno-3-12 name=__codelineno-3-12></a><a href=#__codelineno-3-12><span class=linenos data-linenos="12 "></span></a>    `// 3.映射，把每个单词映射成一个二元组，第二个元素为1，方便后面直接使用reduceByKey`
</span><span id=__span-3-13><a id=__codelineno-3-13 name=__codelineno-3-13></a><a href=#__codelineno-3-13><span class=linenos data-linenos="13 "></span></a> `// hello 变成 (hello, 1)`
</span><span id=__span-3-14><a id=__codelineno-3-14 name=__codelineno-3-14></a><a href=#__codelineno-3-14><span class=linenos data-linenos="14 "></span></a> `val wordAndOne: RDD[(String, Int)] = words.map((_, 1))`
</span><span id=__span-3-15><a id=__codelineno-3-15 name=__codelineno-3-15></a><a href=#__codelineno-3-15><span class=linenos data-linenos="15 "></span></a> `// 4.分组聚合两步操作直接合并在一起，也就是对所有key相同的二元组的value直接做reduce操作`
</span><span id=__span-3-16><a id=__codelineno-3-16 name=__codelineno-3-16></a><a href=#__codelineno-3-16><span class=linenos data-linenos="16 "></span></a> `val wordToCount: RDD[(String, Int)] = wordAndOne.reduceByKey(_ + _)`
</span><span id=__span-3-17><a id=__codelineno-3-17 name=__codelineno-3-17></a><a href=#__codelineno-3-17><span class=linenos data-linenos="17 "></span></a>    // 5. 当调用collect时才会进行数据的计算。也就是说上面的步骤只是在声明操作，并没有真正的计算，只有执行collect方法时数据才会开始流动进行上面的操作
</span><span id=__span-3-18><a id=__codelineno-3-18 name=__codelineno-3-18></a><a href=#__codelineno-3-18><span class=linenos data-linenos="18 "></span></a>    // collect的返回值是Array，也就是最后会把RDD转换成Scala的数据结构，方便输出结果
</span><span id=__span-3-19><a id=__codelineno-3-19 name=__codelineno-3-19></a><a href=#__codelineno-3-19><span class=linenos data-linenos="19 "></span></a>    // Array((hello -&gt; 3), (world -&gt; 2))
</span><span id=__span-3-20><a id=__codelineno-3-20 name=__codelineno-3-20></a><a href=#__codelineno-3-20><span class=linenos data-linenos="20 "></span></a>    val array: Array[(String, Int)] = wordToCount.collect()
</span><span id=__span-3-21><a id=__codelineno-3-21 name=__codelineno-3-21></a><a href=#__codelineno-3-21><span class=linenos data-linenos="21 "></span></a>    array.foreach(println)
</span><span id=__span-3-22><a id=__codelineno-3-22 name=__codelineno-3-22></a><a href=#__codelineno-3-22><span class=linenos data-linenos="22 "></span></a>    // TODO 关闭连接
</span><span id=__span-3-23><a id=__codelineno-3-23 name=__codelineno-3-23></a><a href=#__codelineno-3-23><span class=linenos data-linenos="23 "></span></a>    sc.stop()
</span><span id=__span-3-24><a id=__codelineno-3-24 name=__codelineno-3-24></a><a href=#__codelineno-3-24><span class=linenos data-linenos="24 "></span></a>  }
</span><span id=__span-3-25><a id=__codelineno-3-25 name=__codelineno-3-25></a><a href=#__codelineno-3-25><span class=linenos data-linenos="25 "></span></a>}
</span></code></pre></div></p> <p><strong>Local 模式</strong> 1.解压缩Spark并重命名 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1></a><a href=#__codelineno-4-1><span class=linenos data-linenos="1 "></span></a>tar -zxvf /opt/software/spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module/
</span></code></pre></div> <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1></a><a href=#__codelineno-5-1><span class=linenos data-linenos="1 "></span></a>mv spark-3.0.0-bin-hadoop3.2/ spark-local
</span></code></pre></div> 2.进入Spark的命令行交互界面 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1></a><a href=#__codelineno-6-1><span class=linenos data-linenos="1 "></span></a>bin/spark-shell
</span></code></pre></div> Spark WebUI地址，查看本次会话期间任务运行情况： <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1></a><a href=#__codelineno-7-1><span class=linenos data-linenos="1 "></span></a>http://hadoop102:4040
</span></code></pre></div> 3.测试，使用命令行交互界面完成wordcount程序 3.1 在/opt/module/spark-local/data下新建一个words.txt并添加点内容 3.2 输入如下代码。在命令行中已经创建了SparkContext对象sc，sc使用的master是Local 模式，直接使用即可，所以命令行只能在Local模式下执行spark程序。 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1></a><a href=#__codelineno-8-1><span class=linenos data-linenos="1 "></span></a>sc.textFile(&quot;data/words.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).collect()
</span></code></pre></div> 4. 退出命令行，按键 Ctrl+C 或输入 Scala 指令 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1></a><a href=#__codelineno-9-1><span class=linenos data-linenos="1 "></span></a>:quit
</span></code></pre></div> 5. Local 模式提交应用。还可以将IDEA中的程序打包之后再提交给spark运行，以下是将spark自带的计算圆周率的程序提交给spark运行的命令，spark-submit 提交的Local 模式的任务的日志不能在 http://hadoop102:4040 看到，http://hadoop102:4040 只能看到当前会话的任务的日志，因为会话的app-id与单独提交运行的app-id不同。 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1></a><a href=#__codelineno-10-1><span class=linenos data-linenos="1 "></span></a>bin/spark-submit \
</span><span id=__span-10-2><a id=__codelineno-10-2 name=__codelineno-10-2></a><a href=#__codelineno-10-2><span class=linenos data-linenos="2 "></span></a>--class org.apache.spark.examples.SparkPi \
</span><span id=__span-10-3><a id=__codelineno-10-3 name=__codelineno-10-3></a><a href=#__codelineno-10-3><span class=linenos data-linenos="3 "></span></a>--master local[2] \
</span><span id=__span-10-4><a id=__codelineno-10-4 name=__codelineno-10-4></a><a href=#__codelineno-10-4><span class=linenos data-linenos="4 "></span></a>./examples/jars/spark-examples_2.12-3.0.0.jar \
</span><span id=__span-10-5><a id=__codelineno-10-5 name=__codelineno-10-5></a><a href=#__codelineno-10-5><span class=linenos data-linenos="5 "></span></a>10
</span></code></pre></div></p> <p><strong>Standalone 模式</strong> Spark的Standalone（独立部署）模式指的是使用Spark自带的 资源调度器 部署Spark集群。 集群规划如下，Master类似于Yarn的ResourceManager，Work类似于Yarn的NodeManager，所以这里的集群指的是 资源调度器 的集群，由资源调度器调度spark程序的运行。 <a class=glightbox href=../_resources/1-Spark%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B.resources/unknown_filename.png data-type=image data-width=auto data-height=auto data-title=unknown_filename.png data-desc-position=bottom><img alt=unknown_filename.png src=../_resources/1-Spark%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B.resources/unknown_filename.png></a></p> <p>1.解压缩Spark并重命名 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1></a><a href=#__codelineno-11-1><span class=linenos data-linenos="1 "></span></a>tar -zxvf /opt/software/spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module/
</span></code></pre></div> <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1></a><a href=#__codelineno-12-1><span class=linenos data-linenos="1 "></span></a>mv spark-3.0.0-bin-hadoop3.2/ spark-standalone
</span></code></pre></div> 2.修改 conf/slaves 文件 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1></a><a href=#__codelineno-13-1><span class=linenos data-linenos="1 "></span></a>mv slaves.template slaves
</span></code></pre></div> 添加worker： <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-14-1><a id=__codelineno-14-1 name=__codelineno-14-1></a><a href=#__codelineno-14-1><span class=linenos data-linenos="1 "></span></a>hadoop102
</span><span id=__span-14-2><a id=__codelineno-14-2 name=__codelineno-14-2></a><a href=#__codelineno-14-2><span class=linenos data-linenos="2 "></span></a>hadoop103
</span><span id=__span-14-3><a id=__codelineno-14-3 name=__codelineno-14-3></a><a href=#__codelineno-14-3><span class=linenos data-linenos="3 "></span></a>hadoop104
</span></code></pre></div> 3. 修改 conf/spark-env.sh 文件 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-15-1><a id=__codelineno-15-1 name=__codelineno-15-1></a><a href=#__codelineno-15-1><span class=linenos data-linenos="1 "></span></a>mv spark-env.sh.template spark-env.sh
</span></code></pre></div> 指定 JAVA_HOME 和 master 所在的主机 以及 向 master 的提交spark程序的端口 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-16-1><a id=__codelineno-16-1 name=__codelineno-16-1></a><a href=#__codelineno-16-1><span class=linenos data-linenos="1 "></span></a>export JAVA_HOME=/opt/module/jdk1.8.0_301
</span><span id=__span-16-2><a id=__codelineno-16-2 name=__codelineno-16-2></a><a href=#__codelineno-16-2><span class=linenos data-linenos="2 "></span></a>SPARK_MASTER_HOST=hadoop102
</span><span id=__span-16-3><a id=__codelineno-16-3 name=__codelineno-16-3></a><a href=#__codelineno-16-3><span class=linenos data-linenos="3 "></span></a>SPARK_MASTER_PORT=7077
</span></code></pre></div> 4. 分发 spark <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-17-1><a id=__codelineno-17-1 name=__codelineno-17-1></a><a href=#__codelineno-17-1><span class=linenos data-linenos="1 "></span></a>xsync spark-standalone
</span></code></pre></div> 5. 启动集群 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-18-1><a id=__codelineno-18-1 name=__codelineno-18-1></a><a href=#__codelineno-18-1><span class=linenos data-linenos="1 "></span></a>sbin/start-all.sh
</span></code></pre></div> 查看jvm进程： <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-19-1><a id=__codelineno-19-1 name=__codelineno-19-1></a><a href=#__codelineno-19-1><span class=linenos data-linenos="1 "></span></a>jpsall
</span></code></pre></div> 查看 Master 资源监控 Web UI 界面 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-20-1><a id=__codelineno-20-1 name=__codelineno-20-1></a><a href=#__codelineno-20-1><span class=linenos data-linenos="1 "></span></a>http://hadoop102:8080/
</span></code></pre></div> 6. Standalone 模式提交应用，同样是运行自带的计算圆周率的spark程序，不过这次是提交给Spark自带的资源调度器的master，所以会以分布式计算的方式在多台主机上运行spark程序。 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-21-1><a id=__codelineno-21-1 name=__codelineno-21-1></a><a href=#__codelineno-21-1><span class=linenos data-linenos="1 "></span></a>bin/spark-submit \
</span><span id=__span-21-2><a id=__codelineno-21-2 name=__codelineno-21-2></a><a href=#__codelineno-21-2><span class=linenos data-linenos="2 "></span></a>--class org.apache.spark.examples.SparkPi \
</span><span id=__span-21-3><a id=__codelineno-21-3 name=__codelineno-21-3></a><a href=#__codelineno-21-3><span class=linenos data-linenos="3 "></span></a>`--master spark://hadoop102:7077 \`
</span><span id=__span-21-4><a id=__codelineno-21-4 name=__codelineno-21-4></a><a href=#__codelineno-21-4><span class=linenos data-linenos="4 "></span></a>./examples/jars/spark-examples_2.12-3.0.0.jar \
</span><span id=__span-21-5><a id=__codelineno-21-5 name=__codelineno-21-5></a><a href=#__codelineno-21-5><span class=linenos data-linenos="5 "></span></a>10
</span></code></pre></div> 7.停止集群 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-22-1><a id=__codelineno-22-1 name=__codelineno-22-1></a><a href=#__codelineno-22-1><span class=linenos data-linenos="1 "></span></a>sbin/stop-all.sh
</span></code></pre></div></p> <p>spark-submit命令参数说明：</p> <table> <thead> <tr> <th></th> <th></th> <th></th> </tr> </thead> <tbody> <tr> <td>参数</td> <td>解释</td> <td>可选值举例</td> </tr> <tr> <td>--class</td> <td>指定Spark 程序中包含主函数的类</td> <td></td> </tr> <tr> <td>--master</td> <td>指定Spark 程序运行的模式(环境)</td> <td>模式： local[*]、 spark://haoop102:7077、Yarn</td> </tr> <tr> <td>--executor-memory 1G</td> <td>指定每个 executor 可用内存为 1G</td> <td>符合集群内存配置即可，具体情况具体分析</td> </tr> <tr> <td>--total-executor-cores 2</td> <td>指定所有executor使用的cpu核数为 2 个</td> <td></td> </tr> <tr> <td>--executor-cores</td> <td>指定每个executor使用的cpu核数</td> <td></td> </tr> <tr> <td>application-jar</td> <td>打包好的应用 jar，包含依赖。这个 URL 在集群中全局可见。 比如 hdfs:// 共享存储系统，如果是file:// path，那么所有的节点的path 都包含同样的 jar</td> <td></td> </tr> <tr> <td>application-arguments</td> <td>传给 main()方法的参数</td> <td></td> </tr> </tbody> </table> <p><strong>配置历史服务</strong> Local模式时，退出命令行之后就无法通过 http://hadoop102:4040 查看任务运行情况了，并且任务运行的日志也不会被保存下来，也就是即使再次开启命令行也看不到之前任务的运行日志。Standalone 模式也是一样，只能查看在集群运行期间运行的任务的日志，集群停止之后就查看不到了，并且也不会保留任务的日志。历史服务器可以保存无论是Local模式还是Standalone 模式的任务的日志。 1.修改 conf/spark-defaults.conf 文件 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-23-1><a id=__codelineno-23-1 name=__codelineno-23-1></a><a href=#__codelineno-23-1><span class=linenos data-linenos="1 "></span></a>mv spark-defaults.conf.template spark-defaults.conf
</span></code></pre></div> 添加以下内容，开启日志，并设置将日志保存在HDFS上 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-24-1><a id=__codelineno-24-1 name=__codelineno-24-1></a><a href=#__codelineno-24-1><span class=linenos data-linenos="1 "></span></a>spark.eventLog.enabled true
</span><span id=__span-24-2><a id=__codelineno-24-2 name=__codelineno-24-2></a><a href=#__codelineno-24-2><span class=linenos data-linenos="2 "></span></a>spark.eventLog.dir hdfs://hadoop102:8020/spark-history
</span></code></pre></div> 注意：需要启动 hadoop 集群， HDFS 上的 /spark-history 目录需要提前存在 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-25-1><a id=__codelineno-25-1 name=__codelineno-25-1></a><a href=#__codelineno-25-1><span class=linenos data-linenos="1 "></span></a>sbin/start-dfs.sh
</span><span id=__span-25-2><a id=__codelineno-25-2 name=__codelineno-25-2></a><a href=#__codelineno-25-2><span class=linenos data-linenos="2 "></span></a>hadoop fs -mkdir /spark-history
</span></code></pre></div> 2. 修改 spark-env.sh 文件, 添加日志配置 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-26-1><a id=__codelineno-26-1 name=__codelineno-26-1></a><a href=#__codelineno-26-1><span class=linenos data-linenos="1 "></span></a>export SPARK_HISTORY_OPTS=&quot;
</span><span id=__span-26-2><a id=__codelineno-26-2 name=__codelineno-26-2></a><a href=#__codelineno-26-2><span class=linenos data-linenos="2 "></span></a>-Dspark.history.ui.port=18080
</span><span id=__span-26-3><a id=__codelineno-26-3 name=__codelineno-26-3></a><a href=#__codelineno-26-3><span class=linenos data-linenos="3 "></span></a>-Dspark.history.fs.logDirectory=hdfs://hadoop102:8020/spark-history
</span><span id=__span-26-4><a id=__codelineno-26-4 name=__codelineno-26-4></a><a href=#__codelineno-26-4><span class=linenos data-linenos="4 "></span></a>-Dspark.history.retainedApplications=30&quot;
</span></code></pre></div> -Dspark.history.ui.port： WEB UI 访问的端口号为 18080 -Dspark.history.fs.logDirectory：指定历史服务器日志存储路径 -Dspark.history.retainedApplications：指定保存 Application 历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。 4. 分发配置文件，不分发就只能保存本机Local模式下的日志 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-27-1><a id=__codelineno-27-1 name=__codelineno-27-1></a><a href=#__codelineno-27-1><span class=linenos data-linenos="1 "></span></a>xsync conf
</span></code></pre></div> 5.启动历史服务器 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-28-1><a id=__codelineno-28-1 name=__codelineno-28-1></a><a href=#__codelineno-28-1><span class=linenos data-linenos="1 "></span></a>sbin/start-history-server.sh
</span></code></pre></div> 启动spark集群 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-29-1><a id=__codelineno-29-1 name=__codelineno-29-1></a><a href=#__codelineno-29-1><span class=linenos data-linenos="1 "></span></a>sbin/start-all.sh
</span></code></pre></div> 6.访问历史服务的webui <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-30-1><a id=__codelineno-30-1 name=__codelineno-30-1></a><a href=#__codelineno-30-1><span class=linenos data-linenos="1 "></span></a>http://hadoop102:18080/
</span></code></pre></div> 7.停止历史服务器 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-31-1><a id=__codelineno-31-1 name=__codelineno-31-1></a><a href=#__codelineno-31-1><span class=linenos data-linenos="1 "></span></a>sbin/stop-history-server.sh
</span></code></pre></div></p> <p><strong>Master高可用</strong> 因为 Master 节点只有一个，所以会存在单点故障问题。 为了解决单点故障问题，需要在集群中配置多个 Master 节点，一旦处于活动状态的 Master发生故障时，由备用 Master 提供服务，保证作业可以继续执行。 高可用一般采用Zookeeper 来实现，通过争抢创建某个节点完成主Master的选择。 集群规划： <a class=glightbox href=../_resources/1-Spark%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B.resources/unknown_filename.1.png data-type=image data-width=auto data-height=auto data-title=unknown_filename.1.png data-desc-position=bottom><img alt=unknown_filename.1.png src=../_resources/1-Spark%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B.resources/unknown_filename.1.png></a> 1. 停止spark集群 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-32-1><a id=__codelineno-32-1 name=__codelineno-32-1></a><a href=#__codelineno-32-1><span class=linenos data-linenos="1 "></span></a>sbin/stop-all.sh
</span></code></pre></div> 2. 启动 Zookeeper集群 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-33-1><a id=__codelineno-33-1 name=__codelineno-33-1></a><a href=#__codelineno-33-1><span class=linenos data-linenos="1 "></span></a>zk.sh start
</span></code></pre></div> 3.修改 spark-env.sh 文件 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-34-1><a id=__codelineno-34-1 name=__codelineno-34-1></a><a href=#__codelineno-34-1><span class=linenos data-linenos=" 1 "></span></a>#注释如下内容：
</span><span id=__span-34-2><a id=__codelineno-34-2 name=__codelineno-34-2></a><a href=#__codelineno-34-2><span class=linenos data-linenos=" 2 "></span></a>#SPARK_MASTER_HOST=hadoop102
</span><span id=__span-34-3><a id=__codelineno-34-3 name=__codelineno-34-3></a><a href=#__codelineno-34-3><span class=linenos data-linenos=" 3 "></span></a>#SPARK_MASTER_PORT=7077
</span><span id=__span-34-4><a id=__codelineno-34-4 name=__codelineno-34-4></a><a href=#__codelineno-34-4><span class=linenos data-linenos=" 4 "></span></a>
</span><span id=__span-34-5><a id=__codelineno-34-5 name=__codelineno-34-5></a><a href=#__codelineno-34-5><span class=linenos data-linenos=" 5 "></span></a>#添加如下内容:
</span><span id=__span-34-6><a id=__codelineno-34-6 name=__codelineno-34-6></a><a href=#__codelineno-34-6><span class=linenos data-linenos=" 6 "></span></a>#Master 监控页面默认访问端口为 8080，但是可能会和 Zookeeper 冲突，所以改成 8989，也可以自定义，访问 UI 监控页面时请注意
</span><span id=__span-34-7><a id=__codelineno-34-7 name=__codelineno-34-7></a><a href=#__codelineno-34-7><span class=linenos data-linenos=" 7 "></span></a>SPARK_MASTER_WEBUI_PORT=8989
</span><span id=__span-34-8><a id=__codelineno-34-8 name=__codelineno-34-8></a><a href=#__codelineno-34-8><span class=linenos data-linenos=" 8 "></span></a>export SPARK_DAEMON_JAVA_OPTS=&quot;
</span><span id=__span-34-9><a id=__codelineno-34-9 name=__codelineno-34-9></a><a href=#__codelineno-34-9><span class=linenos data-linenos=" 9 "></span></a>-Dspark.deploy.recoveryMode=ZOOKEEPER
</span><span id=__span-34-10><a id=__codelineno-34-10 name=__codelineno-34-10></a><a href=#__codelineno-34-10><span class=linenos data-linenos="10 "></span></a>-Dspark.deploy.zookeeper.url=hadoop102,hadoop103,hadoop104
</span><span id=__span-34-11><a id=__codelineno-34-11 name=__codelineno-34-11></a><a href=#__codelineno-34-11><span class=linenos data-linenos="11 "></span></a>-Dspark.deploy.zookeeper.dir=/spark&quot;
</span></code></pre></div> 4.分发配置 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-35-1><a id=__codelineno-35-1 name=__codelineno-35-1></a><a href=#__codelineno-35-1><span class=linenos data-linenos="1 "></span></a>xsync conf/spark-env.sh
</span></code></pre></div> 5.启动spark集群 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-36-1><a id=__codelineno-36-1 name=__codelineno-36-1></a><a href=#__codelineno-36-1><span class=linenos data-linenos="1 "></span></a>sbin/start-all.sh
</span></code></pre></div> 6.单独在hadoop103上启动Master，成为备用Master <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-37-1><a id=__codelineno-37-1 name=__codelineno-37-1></a><a href=#__codelineno-37-1><span class=linenos data-linenos="1 "></span></a>sbin/start-master.sh
</span></code></pre></div> 访问hadoop103的Master的webui <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-38-1><a id=__codelineno-38-1 name=__codelineno-38-1></a><a href=#__codelineno-38-1><span class=linenos data-linenos="1 "></span></a>http://hadoop103:8989/
</span></code></pre></div> 可以看到状态是standby <a class=glightbox href=../_resources/1-Spark%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B.resources/unknown_filename.2.png data-type=image data-width=auto data-height=auto data-title=unknown_filename.2.png data-desc-position=bottom><img alt=unknown_filename.2.png src=../_resources/1-Spark%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B.resources/unknown_filename.2.png></a> 7.此时在提交应用时指定两个Master的地址，万一有一个Master挂了，此次提交也不会失败 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-39-1><a id=__codelineno-39-1 name=__codelineno-39-1></a><a href=#__codelineno-39-1><span class=linenos data-linenos="1 "></span></a>bin/spark-submit \
</span><span id=__span-39-2><a id=__codelineno-39-2 name=__codelineno-39-2></a><a href=#__codelineno-39-2><span class=linenos data-linenos="2 "></span></a>--class org.apache.spark.examples.SparkPi \
</span><span id=__span-39-3><a id=__codelineno-39-3 name=__codelineno-39-3></a><a href=#__codelineno-39-3><span class=linenos data-linenos="3 "></span></a>`--master spark://hadoop102:7077,hadoop103:7077 \`
</span><span id=__span-39-4><a id=__codelineno-39-4 name=__codelineno-39-4></a><a href=#__codelineno-39-4><span class=linenos data-linenos="4 "></span></a>./examples/jars/spark-examples_2.12-3.0.0.jar \
</span><span id=__span-39-5><a id=__codelineno-39-5 name=__codelineno-39-5></a><a href=#__codelineno-39-5><span class=linenos data-linenos="5 "></span></a>10
</span></code></pre></div> 8.测试高可用 先杀死hadoop102的master <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-40-1><a id=__codelineno-40-1 name=__codelineno-40-1></a><a href=#__codelineno-40-1><span class=linenos data-linenos="1 "></span></a>kill -9 6652
</span></code></pre></div> 然后查看hadoop103的master的状态，发现变为alive</p> <p><strong>Yarn模式</strong> Standalone模式使用的是Spark自带的资源调度器，与spark-core不是同一个东西，spark-core本身只是一个分布式计算框架，不涉及资源调度，使用spark-core编写的应用程序，可以被资源调度器分散到不同的机器上执行。Yarn作为一款资源调度器，也可以调度Spark程序的运行。 1.解压缩Spark并重命名 注意这里不需要分发spark，因为将spark运行在yarn上，并不需要其它主机也配置spark的环境。也就是将spark程序提交yarn之后，剩下的将任务调度给其它主机的工作是由yarn来完成的，所以只需要有一台主机上有spark的环境即可，这台主机只是用于提交spark程序给yarn。 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-41-1><a id=__codelineno-41-1 name=__codelineno-41-1></a><a href=#__codelineno-41-1><span class=linenos data-linenos="1 "></span></a>tar -zxvf /opt/software/spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module/
</span></code></pre></div> <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-42-1><a id=__codelineno-42-1 name=__codelineno-42-1></a><a href=#__codelineno-42-1><span class=linenos data-linenos="1 "></span></a>mv spark-3.0.0-bin-hadoop3.2/ spark-yarn
</span></code></pre></div> 2. 修改 yarn 的配置文件/opt/module/hadoop-3.3.1/etc/hadoop/yarn-site.xml（不配置也是可以的），并分发 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-43-1><a id=__codelineno-43-1 name=__codelineno-43-1></a><a href=#__codelineno-43-1><span class=linenos data-linenos=" 1 "></span></a>&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是 true --&gt;
</span><span id=__span-43-2><a id=__codelineno-43-2 name=__codelineno-43-2></a><a href=#__codelineno-43-2><span class=linenos data-linenos=" 2 "></span></a>&lt;property&gt;
</span><span id=__span-43-3><a id=__codelineno-43-3 name=__codelineno-43-3></a><a href=#__codelineno-43-3><span class=linenos data-linenos=" 3 "></span></a>    &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;
</span><span id=__span-43-4><a id=__codelineno-43-4 name=__codelineno-43-4></a><a href=#__codelineno-43-4><span class=linenos data-linenos=" 4 "></span></a>    &lt;value&gt;false&lt;/value&gt;
</span><span id=__span-43-5><a id=__codelineno-43-5 name=__codelineno-43-5></a><a href=#__codelineno-43-5><span class=linenos data-linenos=" 5 "></span></a>&lt;/property&gt;
</span><span id=__span-43-6><a id=__codelineno-43-6 name=__codelineno-43-6></a><a href=#__codelineno-43-6><span class=linenos data-linenos=" 6 "></span></a>&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是 true --&gt;
</span><span id=__span-43-7><a id=__codelineno-43-7 name=__codelineno-43-7></a><a href=#__codelineno-43-7><span class=linenos data-linenos=" 7 "></span></a>&lt;property&gt;
</span><span id=__span-43-8><a id=__codelineno-43-8 name=__codelineno-43-8></a><a href=#__codelineno-43-8><span class=linenos data-linenos=" 8 "></span></a>    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;
</span><span id=__span-43-9><a id=__codelineno-43-9 name=__codelineno-43-9></a><a href=#__codelineno-43-9><span class=linenos data-linenos=" 9 "></span></a>    &lt;value&gt;false&lt;/value&gt;
</span><span id=__span-43-10><a id=__codelineno-43-10 name=__codelineno-43-10></a><a href=#__codelineno-43-10><span class=linenos data-linenos="10 "></span></a>&lt;/property&gt;
</span></code></pre></div> <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-44-1><a id=__codelineno-44-1 name=__codelineno-44-1></a><a href=#__codelineno-44-1><span class=linenos data-linenos="1 "></span></a>xsync /opt/module/hadoop-3.3.1/etc/hadoop/yarn-site.xml
</span></code></pre></div> 3. 修改 conf/spark-env.sh，添加 JAVA_HOME 和 YARN_CONF_DIR 配置 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-45-1><a id=__codelineno-45-1 name=__codelineno-45-1></a><a href=#__codelineno-45-1><span class=linenos data-linenos="1 "></span></a>mv spark-env.sh.template spark-env.sh
</span></code></pre></div> <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-46-1><a id=__codelineno-46-1 name=__codelineno-46-1></a><a href=#__codelineno-46-1><span class=linenos data-linenos="1 "></span></a>export JAVA_HOME=/opt/module/jdk1.8.0_301
</span><span id=__span-46-2><a id=__codelineno-46-2 name=__codelineno-46-2></a><a href=#__codelineno-46-2><span class=linenos data-linenos="2 "></span></a>export YARN_CONF_DIR=/opt/module/hadoop-3.3.1/etc/hadoop
</span></code></pre></div> 4.启动Yarn的集群 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-47-1><a id=__codelineno-47-1 name=__codelineno-47-1></a><a href=#__codelineno-47-1><span class=linenos data-linenos="1 "></span></a>start-yarn.sh
</span></code></pre></div> 5.提交spark任务给yarn client模式和cluster部署模式最直观的区别就是：client模式会将结果打印到控制台；而cluster部署模式不会将结果打印到控制台，而是会将输出记录到yarn历史服务器的日志上。 两种模式下真正做计算的Executor都是运行在Yarn集群上，而区别仅在于Driver运行地方，client模式下Driver运行在提交spark程序的本地机器上，Driver的日志和输出的结果都会直接打印到控制台上。而cluster部署模式下Driver 运行在 Yarn 上，Driver输出的日志以及结果会输出到yarn的历史服务器日志中。 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-48-1><a id=__codelineno-48-1 name=__codelineno-48-1></a><a href=#__codelineno-48-1><span class=linenos data-linenos="1 "></span></a>bin/spark-submit \
</span><span id=__span-48-2><a id=__codelineno-48-2 name=__codelineno-48-2></a><a href=#__codelineno-48-2><span class=linenos data-linenos="2 "></span></a>--class org.apache.spark.examples.SparkPi \
</span><span id=__span-48-3><a id=__codelineno-48-3 name=__codelineno-48-3></a><a href=#__codelineno-48-3><span class=linenos data-linenos="3 "></span></a>`--master yarn \`
</span><span id=__span-48-4><a id=__codelineno-48-4 name=__codelineno-48-4></a><a href=#__codelineno-48-4><span class=linenos data-linenos="4 "></span></a>`--deploy-mode client \`
</span><span id=__span-48-5><a id=__codelineno-48-5 name=__codelineno-48-5></a><a href=#__codelineno-48-5><span class=linenos data-linenos="5 "></span></a>./examples/jars/spark-examples_2.12-3.0.0.jar \
</span><span id=__span-48-6><a id=__codelineno-48-6 name=__codelineno-48-6></a><a href=#__codelineno-48-6><span class=linenos data-linenos="6 "></span></a>10
</span></code></pre></div> <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-49-1><a id=__codelineno-49-1 name=__codelineno-49-1></a><a href=#__codelineno-49-1><span class=linenos data-linenos="1 "></span></a>bin/spark-submit \
</span><span id=__span-49-2><a id=__codelineno-49-2 name=__codelineno-49-2></a><a href=#__codelineno-49-2><span class=linenos data-linenos="2 "></span></a>--class org.apache.spark.examples.SparkPi \
</span><span id=__span-49-3><a id=__codelineno-49-3 name=__codelineno-49-3></a><a href=#__codelineno-49-3><span class=linenos data-linenos="3 "></span></a>`--master yarn \`
</span><span id=__span-49-4><a id=__codelineno-49-4 name=__codelineno-49-4></a><a href=#__codelineno-49-4><span class=linenos data-linenos="4 "></span></a>`--deploy-mode cluster \`
</span><span id=__span-49-5><a id=__codelineno-49-5 name=__codelineno-49-5></a><a href=#__codelineno-49-5><span class=linenos data-linenos="5 "></span></a>./examples/jars/spark-examples_2.12-3.0.0.jar \
</span><span id=__span-49-6><a id=__codelineno-49-6 name=__codelineno-49-6></a><a href=#__codelineno-49-6><span class=linenos data-linenos="6 "></span></a>10
</span></code></pre></div></p> <p>6. 配置历史服务器 与前面配置历史服务器步骤差不多，不过这里不需要分发配置。 6.1 修改 conf/spark-defaults.conf 文件 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-50-1><a id=__codelineno-50-1 name=__codelineno-50-1></a><a href=#__codelineno-50-1><span class=linenos data-linenos="1 "></span></a>mv spark-defaults.conf.template spark-defaults.conf
</span></code></pre></div> 添加以下内容，开启日志，并设置将日志保存在HDFS上 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-51-1><a id=__codelineno-51-1 name=__codelineno-51-1></a><a href=#__codelineno-51-1><span class=linenos data-linenos="1 "></span></a>spark.eventLog.enabled true
</span><span id=__span-51-2><a id=__codelineno-51-2 name=__codelineno-51-2></a><a href=#__codelineno-51-2><span class=linenos data-linenos="2 "></span></a>spark.eventLog.dir hdfs://hadoop102:8020/spark-history
</span><span id=__span-51-3><a id=__codelineno-51-3 name=__codelineno-51-3></a><a href=#__codelineno-51-3><span class=linenos data-linenos="3 "></span></a>spark.yarn.historyServer.address=hadoop102:18080
</span><span id=__span-51-4><a id=__codelineno-51-4 name=__codelineno-51-4></a><a href=#__codelineno-51-4><span class=linenos data-linenos="4 "></span></a>spark.history.ui.port=18080
</span></code></pre></div> 注意：需要启动 hadoop 集群， HDFS 上的 /spark-history 目录需要提前存在 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-52-1><a id=__codelineno-52-1 name=__codelineno-52-1></a><a href=#__codelineno-52-1><span class=linenos data-linenos="1 "></span></a>在hadoop的目录下：sbin/start-dfs.sh
</span><span id=__span-52-2><a id=__codelineno-52-2 name=__codelineno-52-2></a><a href=#__codelineno-52-2><span class=linenos data-linenos="2 "></span></a>hadoop fs -mkdir /spark-history
</span></code></pre></div> 6.2 修改 spark-env.sh 文件, 添加日志配置 <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-53-1><a id=__codelineno-53-1 name=__codelineno-53-1></a><a href=#__codelineno-53-1><span class=linenos data-linenos="1 "></span></a>export SPARK_HISTORY_OPTS=&quot;
</span><span id=__span-53-2><a id=__codelineno-53-2 name=__codelineno-53-2></a><a href=#__codelineno-53-2><span class=linenos data-linenos="2 "></span></a>-Dspark.history.ui.port=18080
</span><span id=__span-53-3><a id=__codelineno-53-3 name=__codelineno-53-3></a><a href=#__codelineno-53-3><span class=linenos data-linenos="3 "></span></a>-Dspark.history.fs.logDirectory=hdfs://hadoop102:8020/spark-history
</span><span id=__span-53-4><a id=__codelineno-53-4 name=__codelineno-53-4></a><a href=#__codelineno-53-4><span class=linenos data-linenos="4 "></span></a>-Dspark.history.retainedApplications=30&quot;
</span></code></pre></div> 6.3 启动Spark的历史服务，在Spark的目录下： <div class="language-text highlight"><span class=filename>Text Only</span><pre><span></span><code><span id=__span-54-1><a id=__codelineno-54-1 name=__codelineno-54-1></a><a href=#__codelineno-54-1><span class=linenos data-linenos="1 "></span></a>sbin/start-history-server.sh
</span></code></pre></div> 7.重新提交任务到yarn，查看历史服务器上的日志</p> <p><strong>三种部署模式对比</strong></p> <table> <thead> <tr> <th></th> <th></th> <th></th> </tr> </thead> <tbody> <tr> <td>部署模式</td> <td>Spark安装机器数</td> <td>启动的进程</td> </tr> <tr> <td>Local</td> <td>1</td> <td>通过bin/spark-shell命令启动的SparkSubmit进程，退出命令行SparkSubmit进程就结束了</td> </tr> <tr> <td>Standalone</td> <td>3</td> <td>1个 Master 和 3个分布在不同机器上的 Worker</td> </tr> <tr> <td>Yarn</td> <td>1</td> <td>1个 ResourceManager 和 3个分布在不同机器上的 NodeManager</td> </tr> </tbody> </table> <p>Spark在Local模式下只需要启动一个JVM进程就可以执行Spark程序，虽然不是以分布式的方式在运行程序，但是可以用于测试spark程序。既然Spark是用Scala写的，所以只要有JVM就能运行Spark，于是乎完全<strong>可以在Windows环境下运行Local模式</strong>，不过还是需要Spark提供一些启动脚本，当然Spark也提供了。其实感觉在IDEA里写的Spark程序和Local模式的Spark差不多，说白了Local模式下的Spark只是提供了spark程序运行所依赖的各种jar包，而在IDEA中则是用Maven完成了依赖的引入。</p> <p>注意：需要将Spark解压都无空格的目录</p> <p><strong>端口号</strong></p> <ul> <li>Spark 查看当前 Spark-shell 运行任务情况端口号： 4040（计算）</li> <li>Spark Master 内部通信服务端口号： 7077</li> <li>Standalone 模式下， Spark Master Web 端口号： 8080（资源）</li> <li>Spark 历史服务器端口号： 18080</li> <li>Hadoop YARN 任务运行情况查看端口号： 8088</li> </ul> <nav class="md-footer__inner md-grid" aria-label=页脚> <a href=../../Scala/10-Scala%E7%9A%84%E6%96%B9%E6%B3%95%20%E4%B8%8E%20Java%E7%9A%84%E6%96%B9%E6%B3%95%E5%BC%95%E7%94%A8/ class="md-footer__link md-footer__link--prev" aria-label="上一页: 10-Scala的方法 与 Java的方法引用"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> 上一页 </span> <div class=md-ellipsis> 10-Scala的方法 与 Java的方法引用 </div> </div> </a> <a href=../2-Spark%20%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%20%E5%92%8C%20Yarn%20Client%20%E6%A8%A1%E5%BC%8F/ class="md-footer__link md-footer__link--next" aria-label="下一页: 2-Spark 核心组件 和 Yarn Client 模式"> <div class=md-footer__title> <span class=md-footer__direction> 下一页 </span> <div class=md-ellipsis> 2-Spark 核心组件 和 Yarn Client 模式 </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <h2 id=__comments>评论</h2> <!-- Insert generated snippet here --> <script src=https://giscus.app/client.js data-repo=Jmoon531/Jmoon531.github.io data-repo-id=R_kgDOOKeS9Q data-category=Announcements data-category-id=DIC_kwDOOKeS9c4CoWy3 data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=light data-lang=zh-CN crossorigin=anonymous async>
</script> <!-- Synchronize Giscus theme with palette --> <script>
  var giscus = document.querySelector("script[src*=giscus]")

  // Set palette on initial load
  var palette = __md_get("__palette")
  if (palette && typeof palette.color === "object") {
    var theme = palette.color.scheme === "slate"
      ? "transparent_dark"
      : "light"

    // Instruct Giscus to set theme
    giscus.setAttribute("data-theme", theme)
  }

  // Register event handlers after documented loaded
  document.addEventListener("DOMContentLoaded", function() {
    var ref = document.querySelector("[data-md-component=palette]")
    ref.addEventListener("change", function() {
      var palette = __md_get("__palette")
      if (palette && typeof palette.color === "object") {
        var theme = palette.color.scheme === "slate"
          ? "transparent_dark"
          : "light"

        // Instruct Giscus to change theme
        var frame = document.querySelector(".giscus-frame")
        frame.contentWindow.postMessage(
          { giscus: { setConfig: { theme } } },
          "https://giscus.app"
        )
      }
    })
  })
</script> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> 回到页面顶部 </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright style="max-width: 100%;"> <div class=md-copyright__highlight style="display: grid;grid-template-columns: 1fr 1fr;"> <span>Copyright &copy; 2025 J-moon</span> <span style="justify-self: end;">Contact me via e-mail: Jmoon531@foxmail.com</span> </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../../..", "features": ["navigation.tabs", "navigation.instant", "navigation.instant.progress", "navigation.tracking", "navigation.path", "navigation.top", "navigation.indexes", "navigation.prune", "navigation.footer", "toc.follow", "toc.integrate", "search.suggest", "search.highlight", "search.share", "content.code.copy", "content.code.annotate"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script> <script src=../../../assets/javascripts/bundle.c8b220af.min.js></script> <script src=/assets/javascripts/custom.js></script> <script src=/assets/javascripts/katex.js></script> <script src=https://unpkg.com/katex@0/dist/katex.min.js></script> <script src=https://unpkg.com/katex@0/dist/contrib/auto-render.min.js></script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": false, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>